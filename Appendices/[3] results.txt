*********************************************
Native Language
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 2, 'gamma': 0.01}

0-1 loss 0.767045454545
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.63      0.42      0.51       149
          1       0.80      0.90      0.85       379

avg / total       0.75      0.77      0.75       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.895833333333
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.83      0.80      0.81       149
          1       0.92      0.93      0.93       379

avg / total       0.89      0.90      0.90       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 1, 'gamma': 1}

0-1 loss 0.725378787879
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.75      0.04      0.08       149
          1       0.72      0.99      0.84       379

avg / total       0.73      0.73      0.62       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.88446969697
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.78      0.82      0.80       149
          1       0.93      0.91      0.92       379

avg / total       0.89      0.88      0.89       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.001}

0-1 loss 0.767045454545
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.62      0.44      0.51       149
          1       0.80      0.90      0.85       379

avg / total       0.75      0.77      0.75       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.892045454545
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.83      0.77      0.80       149
          1       0.91      0.94      0.93       379

avg / total       0.89      0.89      0.89       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.888257575758
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.82      0.78      0.80       149
          1       0.91      0.93      0.92       379

avg / total       0.89      0.89      0.89       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.001}

0-1 loss 0.869318181818
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.77      0.77      0.77       149
          1       0.91      0.91      0.91       379

avg / total       0.87      0.87      0.87       528


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 2}

0-1 loss 0.714015151515
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.49      0.47      0.48       149
          1       0.80      0.81      0.80       379

avg / total       0.71      0.71      0.71       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 4}

0-1 loss 0.761363636364
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.60      0.47      0.53       149
          1       0.81      0.88      0.84       379

avg / total       0.75      0.76      0.75       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 1, 'min_samples_leaf': 1}

0-1 loss 0.717803030303
Baseline 0.717803030303

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [0]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [0]. 
  average=None)
             precision    recall  f1-score   support

          0       0.00      0.00      0.00       149
          1       0.72      1.00      0.84       379

avg / total       0.52      0.72      0.60       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 1}

0-1 loss 0.755681818182
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.59      0.44      0.51       149
          1       0.80      0.88      0.84       379

avg / total       0.74      0.76      0.74       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 4}

0-1 loss 0.714015151515
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.49      0.47      0.48       149
          1       0.80      0.81      0.80       379

avg / total       0.71      0.71      0.71       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 2}

0-1 loss 0.767045454545
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.61      0.47      0.53       149
          1       0.81      0.88      0.84       379

avg / total       0.75      0.77      0.76       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': 8, 'min_samples_leaf': 16}

0-1 loss 0.710227272727
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.49      0.54      0.51       149
          1       0.81      0.78      0.79       379

avg / total       0.72      0.71      0.71       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 1}

0-1 loss 0.755681818182
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.59      0.44      0.51       149
          1       0.80      0.88      0.84       379

avg / total       0.74      0.76      0.74       528


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 13}

0-1 loss 0.734848484848
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.57      0.23      0.33       149
          1       0.76      0.93      0.83       379

avg / total       0.70      0.73      0.69       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 13}

0-1 loss 0.801136363636
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.73      0.46      0.57       149
          1       0.82      0.93      0.87       379

avg / total       0.79      0.80      0.79       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 35}

0-1 loss 0.725378787879
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.70      0.05      0.09       149
          1       0.73      0.99      0.84       379

avg / total       0.72      0.73      0.63       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 9}

0-1 loss 0.795454545455
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.69      0.50      0.58       149
          1       0.82      0.91      0.86       379

avg / total       0.79      0.80      0.78       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 17}

0-1 loss 0.751893939394
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.66      0.25      0.36       149
          1       0.76      0.95      0.85       379

avg / total       0.73      0.75      0.71       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 9}

0-1 loss 0.804924242424
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.71      0.52      0.60       149
          1       0.83      0.92      0.87       379

avg / total       0.80      0.80      0.80       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 9}

0-1 loss 0.801136363636
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.72      0.48      0.58       149
          1       0.82      0.93      0.87       379

avg / total       0.79      0.80      0.79       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 3}

0-1 loss 0.823863636364
Baseline 0.717803030303

Detailed classification report:


             precision    recall  f1-score   support

          0       0.73      0.60      0.66       149
          1       0.85      0.91      0.88       379

avg / total       0.82      0.82      0.82       528



*********************************************
Grade
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 1, 'gamma': 0.01}

0-1 loss 0.662239089184
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.55      0.61       248
          1       0.66      0.76      0.70       279

avg / total       0.66      0.66      0.66       527


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 1e-05}

0-1 loss 0.647058823529
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.50      0.57       248
          1       0.64      0.78      0.70       279

avg / total       0.65      0.65      0.64       527


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 1}

0-1 loss 0.586337760911
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.57      0.51      0.54       248
          1       0.60      0.66      0.63       279

avg / total       0.58      0.59      0.58       527


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 0.5, 'gamma': 0.001}

0-1 loss 0.641366223909
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.47      0.55       248
          1       0.63      0.80      0.70       279

avg / total       0.65      0.64      0.63       527


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 1, 'gamma': 0.01}

0-1 loss 0.645161290323
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.65      0.52      0.58       248
          1       0.64      0.75      0.69       279

avg / total       0.65      0.65      0.64       527


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 1e-05}

0-1 loss 0.645161290323
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.66      0.51      0.57       248
          1       0.64      0.77      0.70       279

avg / total       0.65      0.65      0.64       527


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 0.5, 'gamma': 0.001}

0-1 loss 0.639468690702
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.46      0.55       248
          1       0.62      0.80      0.70       279

avg / total       0.65      0.64      0.63       527


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 0.5, 'gamma': 0.001}

0-1 loss 0.671726755218
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.72      0.50      0.59       248
          1       0.65      0.82      0.73       279

avg / total       0.68      0.67      0.66       527


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': 1, 'min_samples_leaf': 1}

0-1 loss 0.593927893738
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.57      0.54      0.56       248
          1       0.61      0.64      0.63       279

avg / total       0.59      0.59      0.59       527


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 4}

0-1 loss 0.614800759013
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.58      0.64      0.61       248
          1       0.65      0.59      0.62       279

avg / total       0.62      0.61      0.62       527


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 2, 'min_samples_leaf': 8}

0-1 loss 0.605313092979
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.32      0.43       248
          1       0.59      0.86      0.70       279

avg / total       0.62      0.61      0.57       527


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': 2, 'min_samples_leaf': 1}

0-1 loss 0.622390891841
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.38      0.49       248
          1       0.60      0.84      0.70       279

avg / total       0.64      0.62      0.60       527


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 1, 'min_samples_leaf': 1}

0-1 loss 0.593927893738
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.57      0.54      0.56       248
          1       0.61      0.64      0.63       279

avg / total       0.59      0.59      0.59       527


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 1}

0-1 loss 0.612903225806
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.34      0.45       248
          1       0.59      0.85      0.70       279

avg / total       0.63      0.61      0.58       527


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': 2, 'min_samples_leaf': 1}

0-1 loss 0.622390891841
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.38      0.49       248
          1       0.60      0.84      0.70       279

avg / total       0.64      0.62      0.60       527


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 2, 'min_samples_leaf': 1}

0-1 loss 0.622390891841
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.38      0.49       248
          1       0.60      0.84      0.70       279

avg / total       0.64      0.62      0.60       527


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 65}

0-1 loss 0.631878557875
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.63      0.52      0.57       248
          1       0.63      0.73      0.68       279

avg / total       0.63      0.63      0.63       527


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 17}

0-1 loss 0.622390891841
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.60      0.58      0.59       248
          1       0.64      0.66      0.65       279

avg / total       0.62      0.62      0.62       527


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 9}

0-1 loss 0.552182163188
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.52      0.52      0.52       248
          1       0.58      0.58      0.58       279

avg / total       0.55      0.55      0.55       527


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 9}

0-1 loss 0.618595825427
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.60      0.56      0.58       248
          1       0.63      0.67      0.65       279

avg / total       0.62      0.62      0.62       527


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 65}

0-1 loss 0.637571157495
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.64      0.53      0.58       248
          1       0.64      0.73      0.68       279

avg / total       0.64      0.64      0.63       527


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 35}

0-1 loss 0.618595825427
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.61      0.53      0.57       248
          1       0.63      0.70      0.66       279

avg / total       0.62      0.62      0.62       527


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 17}

0-1 loss 0.637571157495
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.62      0.60      0.61       248
          1       0.65      0.67      0.66       279

avg / total       0.64      0.64      0.64       527


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 35}

0-1 loss 0.667931688805
Baseline 0.529411764706

Detailed classification report:


             precision    recall  f1-score   support

          0       0.67      0.57      0.62       248
          1       0.66      0.76      0.71       279

avg / total       0.67      0.67      0.66       527



*********************************************
Academic level
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.01}

0-1 loss 0.486742424242
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.50      0.60      0.55       156
          2       0.44      0.52      0.48       149
          3       0.37      0.22      0.28       105
          4       0.60      0.53      0.56       118

avg / total       0.48      0.49      0.48       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.01}

0-1 loss 0.439393939394
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.39      0.67      0.49       156
          2       0.35      0.31      0.33       149
          3       0.57      0.23      0.33       105
          4       0.68      0.49      0.57       118

avg / total       0.48      0.44      0.43       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 128, 'gamma': 0.01}

0-1 loss 0.386363636364
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.39      0.63      0.48       156
          2       0.34      0.47      0.40       149
          3       0.60      0.03      0.05       105
          4       0.47      0.28      0.35       118

avg / total       0.44      0.39      0.34       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 2, 'gamma': 0.001}

0-1 loss 0.469696969697
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.47      0.61      0.53       156
          2       0.39      0.45      0.42       149
          3       0.38      0.17      0.24       105
          4       0.62      0.58      0.60       118

avg / total       0.47      0.47      0.46       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.01}

0-1 loss 0.482954545455
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.49      0.60      0.54       156
          2       0.44      0.51      0.47       149
          3       0.39      0.23      0.29       105
          4       0.60      0.52      0.55       118

avg / total       0.48      0.48      0.47       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 2, 'gamma': 0.001}

0-1 loss 0.462121212121
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.49      0.60      0.54       156
          2       0.37      0.44      0.40       149
          3       0.39      0.16      0.23       105
          4       0.58      0.58      0.58       118

avg / total       0.46      0.46      0.45       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 2, 'gamma': 0.001}

0-1 loss 0.473484848485
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.47      0.61      0.53       156
          2       0.40      0.45      0.42       149
          3       0.39      0.18      0.25       105
          4       0.63      0.58      0.61       118

avg / total       0.47      0.47      0.46       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.01}

0-1 loss 0.505681818182
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.74      0.57       156
          2       0.44      0.46      0.45       149
          3       0.52      0.24      0.33       105
          4       0.78      0.49      0.60       118

avg / total       0.54      0.51      0.49       528


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': 2, 'min_samples_leaf': 1}

0-1 loss 0.397727272727
Baseline 0.295454545455

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [3]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [3]. 
  average=None)
             precision    recall  f1-score   support

          1       0.39      0.71      0.51       156
          2       0.35      0.41      0.38       149
          3       0.00      0.00      0.00       105
          4       0.53      0.32      0.40       118

avg / total       0.33      0.40      0.35       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': 8, 'min_samples_leaf': 8}

0-1 loss 0.340909090909
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.36      0.33      0.35       156
          2       0.31      0.54      0.39       149
          3       0.22      0.10      0.13       105
          4       0.48      0.32      0.39       118

avg / total       0.34      0.34      0.33       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 8}

0-1 loss 0.373106060606
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.38      0.67      0.49       156
          2       0.35      0.36      0.35       149
          3       0.60      0.03      0.05       105
          4       0.38      0.31      0.34       118

avg / total       0.41      0.37      0.33       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 32}

0-1 loss 0.36553030303
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.42      0.44       156
          2       0.33      0.42      0.37       149
          3       0.29      0.05      0.08       105
          4       0.34      0.51      0.40       118

avg / total       0.36      0.37      0.34       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 2}

0-1 loss 0.410984848485
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.48      0.47       156
          2       0.37      0.51      0.43       149
          3       0.33      0.05      0.08       105
          4       0.43      0.52      0.47       118

avg / total       0.40      0.41      0.38       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 16}

0-1 loss 0.397727272727
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.40      0.64      0.50       156
          2       0.38      0.40      0.39       149
          3       0.00      0.00      0.00       105
          4       0.41      0.42      0.41       118

avg / total       0.32      0.40      0.35       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 32}

0-1 loss 0.36553030303
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.42      0.44       156
          2       0.33      0.42      0.37       149
          3       0.29      0.05      0.08       105
          4       0.34      0.51      0.40       118

avg / total       0.36      0.37      0.34       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 4, 'min_samples_leaf': 32}

0-1 loss 0.36553030303
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.42      0.44       156
          2       0.33      0.42      0.37       149
          3       0.29      0.05      0.08       105
          4       0.34      0.51      0.40       118

avg / total       0.36      0.37      0.34       528


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 5}

0-1 loss 0.424242424242
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.53      0.49       156
          2       0.35      0.47      0.40       149
          3       0.38      0.22      0.28       105
          4       0.57      0.42      0.48       118

avg / total       0.44      0.42      0.42       528


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 3}

0-1 loss 0.397727272727
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.48      0.38      0.42       156
          2       0.32      0.38      0.35       149
          3       0.33      0.32      0.33       105
          4       0.50      0.51      0.50       118

avg / total       0.41      0.40      0.40       528


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 65}

0-1 loss 0.371212121212
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.40      0.69      0.51       156
          2       0.32      0.32      0.32       149
          3       0.40      0.04      0.07       105
          4       0.36      0.31      0.33       118

avg / total       0.37      0.37      0.33       528


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.456439393939
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.55      0.31      0.40       156
          2       0.41      0.48      0.44       149
          3       0.40      0.45      0.42       105
          4       0.50      0.62      0.55       118

avg / total       0.47      0.46      0.45       528


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 5}

0-1 loss 0.443181818182
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.46      0.59      0.52       156
          2       0.38      0.46      0.41       149
          3       0.40      0.26      0.31       105
          4       0.58      0.40      0.47       118

avg / total       0.45      0.44      0.44       528


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.424242424242
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.62      0.32      0.42       156
          2       0.36      0.40      0.38       149
          3       0.34      0.41      0.37       105
          4       0.47      0.60      0.53       118

avg / total       0.45      0.42      0.42       528


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 3}

0-1 loss 0.448863636364
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.51      0.49      0.50       156
          2       0.38      0.48      0.42       149
          3       0.35      0.29      0.31       105
          4       0.58      0.50      0.54       118

avg / total       0.46      0.45      0.45       528


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.543560606061
Baseline 0.295454545455

Detailed classification report:


             precision    recall  f1-score   support

          1       0.68      0.46      0.55       156
          2       0.46      0.56      0.51       149
          3       0.46      0.48      0.47       105
          4       0.62      0.69      0.65       118

avg / total       0.56      0.54      0.54       528



*********************************************
Author: 20
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.001}

0-1 loss 0.5
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       1.00      0.50      0.67         2
         75       1.00      0.50      0.67         2
        137       0.50      0.50      0.50         2
        202       0.33      0.50      0.40         2
        244       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.20      0.50      0.29         2
        258       0.20      0.50      0.29         2
        311       0.50      0.50      0.50         2
       3006       0.50      0.50      0.50         2
       3013       1.00      1.00      1.00         2
       6008       1.00      0.50      0.67         2
       6131       0.50      0.50      0.50         2
       6159       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6180       0.50      0.50      0.50         2
       6197       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.53      0.50      0.49        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 0.001}

0-1 loss 0.8
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      1.00      1.00         2
         75       0.00      0.00      0.00         2
        137       0.67      1.00      0.80         2
        202       1.00      0.50      0.67         2
        244       0.40      1.00      0.57         2
        249       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        258       1.00      1.00      1.00         2
        311       0.25      0.50      0.33         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       0.67      1.00      0.80         2
       6131       1.00      0.50      0.67         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.80      0.80      0.78        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 64, 'gamma': 0.1}

0-1 loss 0.325
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       0.40      1.00      0.57         2
         29       0.00      0.00      0.00         2
         75       0.67      1.00      0.80         2
        137       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        244       0.33      0.50      0.40         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.33      0.50      0.40         2
       3006       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       6008       0.25      0.50      0.33         2
       6131       0.00      0.00      0.00         2
       6159       0.33      0.50      0.40         2
       6171       1.00      0.50      0.67         2
       6180       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6210       0.67      1.00      0.80         2
       6211       1.00      0.50      0.67         2

avg / total       0.30      0.33      0.29        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.775
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      1.00      1.00         2
         75       1.00      0.50      0.67         2
        137       1.00      0.50      0.67         2
        202       1.00      0.50      0.67         2
        244       0.33      0.50      0.40         2
        249       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        258       0.29      1.00      0.44         2
        311       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       0.50      1.00      0.67         2
       6131       1.00      1.00      1.00         2
       6159       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.86      0.78      0.78        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 64, 'gamma': 0.001}

0-1 loss 0.55
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      0.50      0.67         2
         75       1.00      0.50      0.67         2
        137       0.50      0.50      0.50         2
        202       0.50      1.00      0.67         2
        244       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        258       0.20      0.50      0.29         2
        311       0.67      1.00      0.80         2
       3006       0.50      0.50      0.50         2
       3013       1.00      1.00      1.00         2
       6008       0.50      0.50      0.50         2
       6131       0.50      0.50      0.50         2
       6159       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6180       0.50      0.50      0.50         2
       6197       1.00      0.50      0.67         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.58      0.55      0.54        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 0.001}

0-1 loss 0.85
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         29       1.00      1.00      1.00         2
         75       0.50      0.50      0.50         2
        137       1.00      1.00      1.00         2
        202       1.00      0.50      0.67         2
        244       0.40      1.00      0.57         2
        249       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        258       1.00      1.00      1.00         2
        311       0.50      0.50      0.50         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       0.67      1.00      0.80         2
       6131       1.00      0.50      0.67         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.90      0.85      0.85        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.8
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      1.00      1.00         2
         75       1.00      1.00      1.00         2
        137       1.00      0.50      0.67         2
        202       1.00      0.50      0.67         2
        244       0.50      0.50      0.50         2
        249       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        258       0.29      1.00      0.44         2
        311       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       0.50      1.00      0.67         2
       6131       1.00      1.00      1.00         2
       6159       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.86      0.80      0.80        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 0.001}

0-1 loss 0.95
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       0.67      1.00      0.80         2
         75       1.00      1.00      1.00         2
        137       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        244       1.00      1.00      1.00         2
        249       1.00      1.00      1.00         2
        252       1.00      0.50      0.67         2
        258       1.00      1.00      1.00         2
        311       0.67      1.00      0.80         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       1.00      1.00      1.00         2
       6131       1.00      1.00      1.00         2
       6159       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.97      0.95      0.95        40


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': 16, 'min_samples_leaf': 1}

0-1 loss 0.275
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [3006 6008 6171]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  75  137  252  258 3006 6008 6131 6171 6197 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         29       0.33      0.50      0.40         2
         75       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        202       0.33      0.50      0.40         2
        244       0.50      0.50      0.50         2
        249       0.25      0.50      0.33         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.50      1.00      0.67         2
       3006       0.00      0.00      0.00         2
       3013       0.33      0.50      0.40         2
       6008       0.00      0.00      0.00         2
       6131       0.00      0.00      0.00         2
       6159       0.50      0.50      0.50         2
       6171       0.00      0.00      0.00         2
       6180       0.33      0.50      0.40         2
       6197       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       0.50      0.50      0.50         2

avg / total       0.23      0.28      0.24        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': 16, 'min_samples_leaf': 1}

0-1 loss 0.35
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  29 6008]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  29  244  252  258  311 3006 3013 6008 6159]. 
  average=None)
             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       0.00      0.00      0.00         2
         75       0.33      0.50      0.40         2
        137       0.67      1.00      0.80         2
        202       1.00      0.50      0.67         2
        244       0.00      0.00      0.00         2
        249       0.50      0.50      0.50         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6131       0.33      0.50      0.40         2
       6159       0.00      0.00      0.00         2
       6171       0.50      1.00      0.67         2
       6180       1.00      0.50      0.67         2
       6197       0.33      0.50      0.40         2
       6210       0.50      0.50      0.50         2
       6211       0.50      0.50      0.50         2

avg / total       0.32      0.35      0.32        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 4}

0-1 loss 0.225
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 258 3006 3013 6131 6159 6180 6211]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 137  202  244  252  258 3006 3013 6131 6159 6171 6180 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.33      0.50      0.40         2
         29       0.33      0.50      0.40         2
         75       0.50      0.50      0.50         2
        137       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        249       0.25      0.50      0.33         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.20      0.50      0.29         2
       3006       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       6008       0.25      1.00      0.40         2
       6131       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6210       0.33      0.50      0.40         2
       6211       0.00      0.00      0.00         2

avg / total       0.16      0.23      0.17        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 1}

0-1 loss 0.5
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 202  258 6180]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 202  258  311 6159 6180]. 
  average=None)
             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       0.50      0.50      0.50         2
         75       0.50      0.50      0.50         2
        137       1.00      1.00      1.00         2
        202       0.00      0.00      0.00         2
        244       0.50      0.50      0.50         2
        249       0.50      0.50      0.50         2
        252       0.50      0.50      0.50         2
        258       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
       3006       0.50      1.00      0.67         2
       3013       0.25      0.50      0.33         2
       6008       1.00      0.50      0.67         2
       6131       1.00      0.50      0.67         2
       6159       0.00      0.00      0.00         2
       6171       0.67      1.00      0.80         2
       6180       0.00      0.00      0.00         2
       6197       0.67      1.00      0.80         2
       6210       0.50      0.50      0.50         2
       6211       0.33      0.50      0.40         2

avg / total       0.45      0.50      0.46        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 1}

0-1 loss 0.425
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [6197 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 137  252 6131 6197 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       0.50      0.50      0.50         2
         29       1.00      0.50      0.67         2
         75       0.50      0.50      0.50         2
        137       0.00      0.00      0.00         2
        202       0.50      0.50      0.50         2
        244       1.00      0.50      0.67         2
        249       0.33      0.50      0.40         2
        252       0.00      0.00      0.00         2
        258       1.00      0.50      0.67         2
        311       0.50      1.00      0.67         2
       3006       0.67      1.00      0.80         2
       3013       0.25      0.50      0.33         2
       6008       0.50      0.50      0.50         2
       6131       0.00      0.00      0.00         2
       6159       0.33      0.50      0.40         2
       6171       0.33      0.50      0.40         2
       6180       0.33      0.50      0.40         2
       6197       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.44      0.42      0.40        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 16, 'min_samples_leaf': 1}

0-1 loss 0.4
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 244 6008]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  29  244  258  311 3013 6008 6159]. 
  average=None)
             precision    recall  f1-score   support

          3       0.50      0.50      0.50         2
         29       0.00      0.00      0.00         2
         75       0.50      0.50      0.50         2
        137       1.00      1.00      1.00         2
        202       0.67      1.00      0.80         2
        244       0.00      0.00      0.00         2
        249       0.50      0.50      0.50         2
        252       0.50      0.50      0.50         2
        258       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
       3006       0.20      0.50      0.29         2
       3013       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6131       0.50      0.50      0.50         2
       6159       0.00      0.00      0.00         2
       6171       0.50      1.00      0.67         2
       6180       0.50      0.50      0.50         2
       6197       1.00      0.50      0.67         2
       6210       1.00      0.50      0.67         2
       6211       0.25      0.50      0.33         2

avg / total       0.38      0.40      0.37        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 1}

0-1 loss 0.5
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 258 3013 6008]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 244  258  311 3013 6008 6159]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       0.67      1.00      0.80         2
         75       1.00      0.50      0.67         2
        137       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        244       0.00      0.00      0.00         2
        249       0.50      0.50      0.50         2
        252       0.50      0.50      0.50         2
        258       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
       3006       0.33      1.00      0.50         2
       3013       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6131       0.50      0.50      0.50         2
       6159       0.00      0.00      0.00         2
       6171       0.50      1.00      0.67         2
       6180       0.50      0.50      0.50         2
       6197       0.50      0.50      0.50         2
       6210       0.50      0.50      0.50         2
       6211       0.50      0.50      0.50         2

avg / total       0.45      0.50      0.46        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 16, 'min_samples_leaf': 1}

0-1 loss 0.425
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 244 6008 6131 6159]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 244  258  311 3006 6008 6131 6159]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         29       1.00      0.50      0.67         2
         75       0.33      0.50      0.40         2
        137       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        244       0.00      0.00      0.00         2
        249       1.00      0.50      0.67         2
        252       0.50      0.50      0.50         2
        258       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3013       0.50      0.50      0.50         2
       6008       0.00      0.00      0.00         2
       6131       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.50      1.00      0.67         2
       6180       0.50      0.50      0.50         2
       6197       0.67      1.00      0.80         2
       6210       0.50      0.50      0.50         2
       6211       0.50      0.50      0.50         2

avg / total       0.45      0.42      0.42        40


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.425
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  29 6008 6180 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  29  244  249  252 6008 6180 6197 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       0.00      0.00      0.00         2
         75       0.50      0.50      0.50         2
        137       1.00      0.50      0.67         2
        202       0.20      0.50      0.29         2
        244       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        258       0.40      1.00      0.57         2
        311       0.67      1.00      0.80         2
       3006       0.33      0.50      0.40         2
       3013       1.00      1.00      1.00         2
       6008       0.00      0.00      0.00         2
       6131       0.50      0.50      0.50         2
       6159       0.50      0.50      0.50         2
       6171       0.67      1.00      0.80         2
       6180       0.00      0.00      0.00         2
       6197       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.37      0.42      0.37        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.35
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  75  202 6008 6159 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  75  137  202  252  258 6008 6159 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       0.50      0.50      0.50         2
         29       0.20      0.50      0.29         2
         75       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        244       0.17      0.50      0.25         2
        249       1.00      0.50      0.67         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.11      0.50      0.18         2
       3006       1.00      0.50      0.67         2
       3013       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       0.50      0.50      0.50         2
       6197       1.00      0.50      0.67         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.41      0.35      0.34        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.225
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [3006 6171 6180]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  29   75  137  202  249  252 3006 3013 6131 6159 6171 6180]. 
  average=None)
             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       0.00      0.00      0.00         2
         75       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        244       0.33      0.50      0.40         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        258       0.25      0.50      0.33         2
        311       0.33      0.50      0.40         2
       3006       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       6008       0.20      0.50      0.29         2
       6131       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6210       0.50      0.50      0.50         2
       6211       0.50      0.50      0.50         2

avg / total       0.19      0.23      0.19        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.525
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 202 6159 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 202  258 6008 6159 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      0.50      0.67         2
         75       0.33      0.50      0.40         2
        137       0.67      1.00      0.80         2
        202       0.00      0.00      0.00         2
        244       0.20      0.50      0.29         2
        249       1.00      0.50      0.67         2
        252       0.33      0.50      0.40         2
        258       0.00      0.00      0.00         2
        311       0.33      1.00      0.50         2
       3006       0.50      0.50      0.50         2
       3013       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6131       0.33      0.50      0.40         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       1.00      0.50      0.67         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.53      0.53      0.50        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.45
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [6008 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 249  252 6008 6131 6159 6197 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         29       1.00      0.50      0.67         2
         75       0.33      0.50      0.40         2
        137       0.67      1.00      0.80         2
        202       0.25      0.50      0.33         2
        244       0.50      0.50      0.50         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        258       0.33      0.50      0.40         2
        311       0.50      1.00      0.67         2
       3006       0.33      0.50      0.40         2
       3013       1.00      1.00      1.00         2
       6008       0.00      0.00      0.00         2
       6131       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.67      1.00      0.80         2
       6180       1.00      0.50      0.67         2
       6197       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.41      0.45      0.41        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.4
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  75  202 6008 6159 6210]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  75  202  252  258 6008 6159 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         29       0.20      0.50      0.29         2
         75       0.00      0.00      0.00         2
        137       0.33      0.50      0.40         2
        202       0.00      0.00      0.00         2
        244       0.17      0.50      0.25         2
        249       0.50      0.50      0.50         2
        252       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        311       0.14      0.50      0.22         2
       3006       0.50      0.50      0.50         2
       3013       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       0.50      0.50      0.50         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.40      0.40      0.37        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.525
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         29       1.00      0.50      0.67         2
         75       0.33      0.50      0.40         2
        137       0.67      1.00      0.80         2
        202       0.00      0.00      0.00         2
        244       0.20      0.50      0.29         2
        249       1.00      0.50      0.67         2
        252       0.50      0.50      0.50         2
        258       0.00      0.00      0.00         2
        311       0.33      1.00      0.50         2
       3006       0.33      0.50      0.40         2
       3013       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6131       0.33      0.50      0.40         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.53      0.53      0.50        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.85
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       0.67      1.00      0.80         2
         75       0.67      1.00      0.80         2
        137       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        244       1.00      1.00      1.00         2
        249       0.50      0.50      0.50         2
        252       1.00      0.50      0.67         2
        258       1.00      1.00      1.00         2
        311       0.50      1.00      0.67         2
       3006       1.00      1.00      1.00         2
       3013       1.00      0.50      0.67         2
       6008       1.00      0.50      0.67         2
       6131       1.00      0.50      0.67         2
       6159       1.00      0.50      0.67         2
       6171       0.67      1.00      0.80         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.90      0.85      0.84        40



*********************************************
Author: 20, Art & Humanities
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.6
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       0.67      1.00      0.80         2
         29       0.14      0.50      0.22         2
         61       0.00      0.00      0.00         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       0.33      0.50      0.40         2
        280       0.00      0.00      0.00         2
        294       0.50      1.00      0.67         2
        303       1.00      1.00      1.00         2
        322       0.00      0.00      0.00         2
       3006       0.50      0.50      0.50         2
       3008       1.00      1.00      1.00         2
       3125       1.00      0.50      0.67         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       1.00      0.50      0.67         2
       6204       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.66      0.60      0.60        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 64, 'gamma': 0.0001}

0-1 loss 0.85
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       1.00      1.00      1.00         2
         29       0.40      1.00      0.57         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       1.00      0.50      0.67         2
        280       0.50      0.50      0.50         2
        294       0.67      1.00      0.80         2
        303       1.00      0.50      0.67         2
        322       0.67      1.00      0.80         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      0.50      0.67         2
       6211       1.00      1.00      1.00         2

avg / total       0.91      0.85      0.85        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 16, 'gamma': 0.1}

0-1 loss 0.225
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       0.50      0.50      0.50         2
         24       0.00      0.00      0.00         2
         29       0.12      0.50      0.20         2
         61       0.33      0.50      0.40         2
        129       0.50      0.50      0.50         2
        130       0.33      0.50      0.40         2
        252       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.33      0.50      0.40         2
       3008       0.00      0.00      0.00         2
       3125       0.33      0.50      0.40         2
       3147       0.33      0.50      0.40         2
       6171       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.19      0.23      0.19        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.0001}

0-1 loss 0.9
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       1.00      1.00      1.00         2
         29       0.40      1.00      0.57         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        322       1.00      1.00      1.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.92      0.90      0.90        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.55
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       0.50      1.00      0.67         2
         29       0.14      0.50      0.22         2
         61       0.00      0.00      0.00         2
        129       1.00      0.50      0.67         2
        130       1.00      0.50      0.67         2
        252       0.33      0.50      0.40         2
        280       0.00      0.00      0.00         2
        294       0.33      0.50      0.40         2
        303       0.67      1.00      0.80         2
        322       0.00      0.00      0.00         2
       3006       0.50      0.50      0.50         2
       3008       1.00      1.00      1.00         2
       3125       0.50      0.50      0.50         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       1.00      0.50      0.67         2
       6204       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.60      0.55      0.54        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.0001}

0-1 loss 0.825
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       1.00      1.00      1.00         2
         29       0.29      1.00      0.44         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       0.67      1.00      0.80         2
        303       1.00      0.50      0.67         2
        322       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      0.50      0.67         2
       6211       1.00      1.00      1.00         2

avg / total       0.90      0.82      0.83        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.85
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       1.00      1.00      1.00         2
         29       0.33      1.00      0.50         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        322       1.00      1.00      1.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       0.67      1.00      0.80         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      0.50      0.67         2
       6211       1.00      1.00      1.00         2

avg / total       0.90      0.85      0.85        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.875
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [280]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [280]. 
  average=None)
             precision    recall  f1-score   support

         19       0.67      1.00      0.80         2
         24       0.67      1.00      0.80         2
         29       0.50      1.00      0.67         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        322       1.00      1.00      1.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.88      0.88      0.85        40


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 1}

0-1 loss 0.325
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19   61 6203 6211]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   61  129  252  294  322 3125 6203 6204 6211]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       1.00      1.00      1.00         2
         29       0.33      0.50      0.40         2
         61       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       0.50      0.50      0.50         2
        252       0.00      0.00      0.00         2
        280       0.20      0.50      0.29         2
        294       0.00      0.00      0.00         2
        303       0.33      1.00      0.50         2
        322       0.00      0.00      0.00         2
       3006       0.50      1.00      0.67         2
       3008       0.33      0.50      0.40         2
       3125       0.00      0.00      0.00         2
       3147       1.00      0.50      0.67         2
       6171       1.00      0.50      0.67         2
       6197       0.50      0.50      0.50         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.29      0.33      0.28        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 4}

0-1 loss 0.425
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  29 3006 6211]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   29  129  130  303 3006 6204 6211]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       0.50      0.50      0.50         2
         29       0.00      0.00      0.00         2
         61       0.50      1.00      0.67         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        252       0.67      1.00      0.80         2
        280       0.29      1.00      0.44         2
        294       0.67      1.00      0.80         2
        303       0.00      0.00      0.00         2
        322       1.00      0.50      0.67         2
       3006       0.00      0.00      0.00         2
       3008       1.00      0.50      0.67         2
       3125       0.50      0.50      0.50         2
       3147       0.33      0.50      0.40         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       1.00      0.50      0.67         2
       6204       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.42      0.42      0.39        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 1}

0-1 loss 0.225
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 303  322 3147 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  29   61  130  280  294  303  322 3125 3147 6171 6203 6204]. 
  average=None)
             precision    recall  f1-score   support

         19       1.00      0.50      0.67         2
         24       0.50      0.50      0.50         2
         29       0.00      0.00      0.00         2
         61       0.00      0.00      0.00         2
        129       0.50      0.50      0.50         2
        130       0.00      0.00      0.00         2
        252       0.50      0.50      0.50         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.14      0.50      0.22         2
       3008       0.25      0.50      0.33         2
       3125       0.00      0.00      0.00         2
       3147       0.00      0.00      0.00         2
       6171       0.00      0.00      0.00         2
       6197       0.33      0.50      0.40         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.21      0.23      0.21        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 2}

0-1 loss 0.3
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 129 3008 3125 3147]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   29   61  129  252  280  303 3008 3125 3147 6211]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       1.00      0.50      0.67         2
         29       0.00      0.00      0.00         2
         61       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       0.40      1.00      0.57         2
        252       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.40      1.00      0.57         2
        303       0.00      0.00      0.00         2
        322       1.00      0.50      0.67         2
       3006       1.00      0.50      0.67         2
       3008       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       0.33      0.50      0.40         2
       6204       0.50      0.50      0.50         2
       6211       0.00      0.00      0.00         2

avg / total       0.33      0.30      0.29        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 4}

0-1 loss 0.4
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [3125 3147 6203 6204]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 252  294  303  322 3125 3147 6203 6204]. 
  average=None)
             precision    recall  f1-score   support

         19       0.50      0.50      0.50         2
         24       1.00      1.00      1.00         2
         29       0.40      1.00      0.57         2
         61       0.33      0.50      0.40         2
        129       0.50      0.50      0.50         2
        130       0.67      1.00      0.80         2
        252       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.33      1.00      0.50         2
       3008       0.50      0.50      0.50         2
       3125       0.00      0.00      0.00         2
       3147       0.00      0.00      0.00         2
       6171       1.00      0.50      0.67         2
       6197       0.50      0.50      0.50         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       0.50      0.50      0.50         2

avg / total       0.33      0.40      0.34        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 1}

0-1 loss 0.4
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19 6204]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   29  129  280  303 6203 6204 6211]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       0.33      0.50      0.40         2
         29       0.00      0.00      0.00         2
         61       1.00      0.50      0.67         2
        129       0.00      0.00      0.00         2
        130       0.50      0.50      0.50         2
        252       1.00      0.50      0.67         2
        280       0.00      0.00      0.00         2
        294       0.67      1.00      0.80         2
        303       0.00      0.00      0.00         2
        322       0.50      0.50      0.50         2
       3006       0.50      0.50      0.50         2
       3008       0.50      1.00      0.67         2
       3125       0.25      0.50      0.33         2
       3147       0.50      0.50      0.50         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.39      0.40      0.38        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 1}

0-1 loss 0.475
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19 3147]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   29  322 3147 6204]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       1.00      0.50      0.67         2
         29       0.00      0.00      0.00         2
         61       0.50      0.50      0.50         2
        129       1.00      0.50      0.67         2
        130       0.67      1.00      0.80         2
        252       1.00      0.50      0.67         2
        280       0.50      1.00      0.67         2
        294       0.50      0.50      0.50         2
        303       0.50      0.50      0.50         2
        322       0.00      0.00      0.00         2
       3006       1.00      0.50      0.67         2
       3008       0.50      0.50      0.50         2
       3125       0.33      0.50      0.40         2
       3147       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6197       0.33      0.50      0.40         2
       6203       0.33      0.50      0.40         2
       6204       0.00      0.00      0.00         2
       6211       0.67      1.00      0.80         2

avg / total       0.49      0.47      0.46        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 16, 'min_samples_leaf': 1}

0-1 loss 0.5
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19 3147 6204]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   61 3125 3147 6203 6204]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       1.00      1.00      1.00         2
         29       0.50      0.50      0.50         2
         61       0.00      0.00      0.00         2
        129       0.33      0.50      0.40         2
        130       0.67      1.00      0.80         2
        252       1.00      0.50      0.67         2
        280       0.50      1.00      0.67         2
        294       0.33      0.50      0.40         2
        303       0.50      0.50      0.50         2
        322       0.40      1.00      0.57         2
       3006       1.00      0.50      0.67         2
       3008       0.50      0.50      0.50         2
       3125       0.00      0.00      0.00         2
       3147       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.49      0.50      0.47        40


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.45
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [19]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   61  252  280  303  322 3006]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       0.67      1.00      0.80         2
         29       0.20      0.50      0.29         2
         61       0.00      0.00      0.00         2
        129       1.00      0.50      0.67         2
        130       0.67      1.00      0.80         2
        252       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.20      0.50      0.29         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.67      1.00      0.80         2
       3125       0.50      0.50      0.50         2
       3147       0.50      0.50      0.50         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       0.67      1.00      0.80         2
       6204       1.00      0.50      0.67         2
       6211       1.00      0.50      0.67         2

avg / total       0.45      0.45      0.42        40


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.475
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 280  303 3006]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  61  280  303  322 3006]. 
  average=None)
             precision    recall  f1-score   support

         19       1.00      0.50      0.67         2
         24       0.50      0.50      0.50         2
         29       0.07      0.50      0.12         2
         61       0.00      0.00      0.00         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        252       0.50      0.50      0.50         2
        280       0.00      0.00      0.00         2
        294       0.33      0.50      0.40         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       0.50      0.50      0.50         2
       6171       1.00      1.00      1.00         2
       6197       1.00      0.50      0.67         2
       6203       1.00      0.50      0.67         2
       6204       1.00      0.50      0.67         2
       6211       1.00      0.50      0.67         2

avg / total       0.57      0.47      0.49        40


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.275
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 303 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  24   29  130  280  294  303  322 3125 6171 6203 6204]. 
  average=None)
             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       0.50      0.50      0.50         2
        129       1.00      0.50      0.67         2
        130       0.00      0.00      0.00         2
        252       0.33      0.50      0.40         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.33      0.50      0.40         2
       3008       0.25      0.50      0.33         2
       3125       0.00      0.00      0.00         2
       3147       0.50      0.50      0.50         2
       6171       0.00      0.00      0.00         2
       6197       0.67      1.00      0.80         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.28      0.28      0.26        40


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.6
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [129 303]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [129 280 303 322]. 
  average=None)
             precision    recall  f1-score   support

         19       1.00      0.50      0.67         2
         24       0.50      0.50      0.50         2
         29       0.17      0.50      0.25         2
         61       0.50      0.50      0.50         2
        129       0.00      0.00      0.00         2
        130       1.00      1.00      1.00         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       0.50      1.00      0.67         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.50      1.00      0.67         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6211       0.50      0.50      0.50         2

avg / total       0.61      0.60      0.58        40


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.45
Baseline 0.05

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [19]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [ 19  61 252 280 303 322]. 
  average=None)
             precision    recall  f1-score   support

         19       0.00      0.00      0.00         2
         24       1.00      1.00      1.00         2
         29       0.20      0.50      0.29         2
         61       0.00      0.00      0.00         2
        129       1.00      0.50      0.67         2
        130       1.00      0.50      0.67         2
        252       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.20      0.50      0.29         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.33      0.50      0.40         2
       3008       0.67      1.00      0.80         2
       3125       0.50      0.50      0.50         2
       3147       1.00      0.50      0.67         2
       6171       0.67      1.00      0.80         2
       6197       0.50      0.50      0.50         2
       6203       1.00      1.00      1.00         2
       6204       1.00      0.50      0.67         2
       6211       0.50      0.50      0.50         2

avg / total       0.48      0.45      0.44        40


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.55
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       0.50      0.50      0.50         2
         29       0.08      0.50      0.13         2
         61       0.00      0.00      0.00         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        252       1.00      0.50      0.67         2
        280       0.00      0.00      0.00         2
        294       0.50      1.00      0.67         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      0.50      0.67         2
       6204       1.00      0.50      0.67         2
       6211       1.00      0.50      0.67         2

avg / total       0.63      0.55      0.56        40


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.6
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      0.50      0.67         2
         24       0.50      0.50      0.50         2
         29       0.14      0.50      0.22         2
         61       0.50      0.50      0.50         2
        129       0.00      0.00      0.00         2
        130       1.00      1.00      1.00         2
        252       1.00      0.50      0.67         2
        280       0.00      0.00      0.00         2
        294       0.50      1.00      0.67         2
        303       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
       3006       0.50      1.00      0.67         2
       3008       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6211       0.67      1.00      0.80         2

avg / total       0.62      0.60      0.58        40


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.675
Baseline 0.05

Detailed classification report:


             precision    recall  f1-score   support

         19       1.00      0.50      0.67         2
         24       0.50      1.00      0.67         2
         29       0.33      1.00      0.50         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        252       0.50      0.50      0.50         2
        280       0.00      0.00      0.00         2
        294       0.33      0.50      0.40         2
        303       0.50      0.50      0.50         2
        322       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       1.00      0.50      0.67         2
       3147       0.67      1.00      0.80         2
       6171       0.67      1.00      0.80         2
       6197       0.67      1.00      0.80         2
       6203       1.00      0.50      0.67         2
       6204       1.00      0.50      0.67         2
       6211       1.00      0.50      0.67         2

avg / total       0.76      0.68      0.67        40



*********************************************
Author: 100
*********************************************

---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'C': 64, 'gamma': 0.001}

0-1 loss 0.445
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       0.50      1.00      0.67         2
         19       0.33      0.50      0.40         2
         20       0.50      0.50      0.50         2
         24       0.50      0.50      0.50         2
         29       0.29      1.00      0.44         2
         61       0.33      1.00      0.50         2
         75       0.67      1.00      0.80         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.17      0.50      0.25         2
        130       0.50      0.50      0.50         2
        135       0.50      0.50      0.50         2
        137       0.25      0.50      0.33         2
        138       0.00      0.00      0.00         2
        139       0.50      0.50      0.50         2
        169       0.00      0.00      0.00         2
        179       0.33      0.50      0.40         2
        188       0.25      0.50      0.33         2
        194       1.00      0.50      0.67         2
        202       0.25      0.50      0.33         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       1.00      0.50      0.67         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.50      0.50      0.50         2
        253       0.00      0.00      0.00         2
        254       1.00      0.50      0.67         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       0.50      0.50      0.50         2
        294       0.33      0.50      0.40         2
        302       1.00      0.50      0.67         2
        303       0.50      0.50      0.50         2
        311       0.25      1.00      0.40         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       0.50      0.50      0.50         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.50      0.50      0.50         2
        415       0.33      0.50      0.40         2
       3001       1.00      0.50      0.67         2
       3006       0.50      0.50      0.50         2
       3008       1.00      0.50      0.67         2
       3013       1.00      0.50      0.67         2
       3019       1.00      0.50      0.67         2
       3032       1.00      1.00      1.00         2
       3042       1.00      0.50      0.67         2
       3064       0.67      1.00      0.80         2
       3091       0.00      0.00      0.00         2
       3092       0.67      1.00      0.80         2
       3125       0.00      0.00      0.00         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       0.33      0.50      0.40         2
       6011       1.00      0.50      0.67         2
       6012       0.33      0.50      0.40         2
       6013       0.00      0.00      0.00         2
       6015       0.50      1.00      0.67         2
       6016       0.50      0.50      0.50         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       0.50      0.50      0.50         2
       6066       1.00      1.00      1.00         2
       6076       1.00      0.50      0.67         2
       6082       1.00      1.00      1.00         2
       6085       0.25      0.50      0.33         2
       6087       1.00      0.50      0.67         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.50      0.50      0.50         2
       6103       0.33      0.50      0.40         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       1.00      1.00      1.00         2
       6124       0.50      0.50      0.50         2
       6129       0.00      0.00      0.00         2
       6131       0.33      0.50      0.40         2
       6136       1.00      0.50      0.67         2
       6145       1.00      0.50      0.67         2
       6159       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6180       0.25      0.50      0.33         2
       6181       0.00      0.00      0.00         2
       6182       0.40      1.00      0.57         2
       6197       0.00      0.00      0.00         2
       6200       0.00      0.00      0.00         2
       6203       1.00      0.50      0.67         2
       6204       1.00      0.50      0.67         2
       6210       0.50      0.50      0.50         2
       6211       1.00      0.50      0.67         2

avg / total       0.47      0.45      0.43       200


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'C': 128, 'gamma': 0.0001}

0-1 loss 0.68
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       1.00      1.00      1.00         2
         29       0.25      1.00      0.40         2
         61       1.00      1.00      1.00         2
         75       0.14      0.50      0.22         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        135       0.67      1.00      0.80         2
        137       1.00      1.00      1.00         2
        138       1.00      0.50      0.67         2
        139       1.00      1.00      1.00         2
        169       1.00      1.00      1.00         2
        179       1.00      0.50      0.67         2
        188       0.67      1.00      0.80         2
        194       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.67      1.00      0.80         2
        234       0.50      0.50      0.50         2
        244       0.25      0.50      0.33         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.50      0.50      0.50         2
        252       1.00      1.00      1.00         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.25      0.50      0.33         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       1.00      1.00      1.00         2
        294       0.50      1.00      0.67         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.08      0.50      0.14         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.67      1.00      0.80         2
        415       1.00      1.00      1.00         2
       3001       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       0.67      1.00      0.80         2
       3032       1.00      1.00      1.00         2
       3042       1.00      1.00      1.00         2
       3064       1.00      1.00      1.00         2
       3091       1.00      0.50      0.67         2
       3092       0.33      0.50      0.40         2
       3125       1.00      1.00      1.00         2
       3147       0.50      0.50      0.50         2
       6005       0.50      0.50      0.50         2
       6006       1.00      0.50      0.67         2
       6008       0.29      1.00      0.44         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.50      0.50      0.50         2
       6013       0.00      0.00      0.00         2
       6015       0.67      1.00      0.80         2
       6016       1.00      0.50      0.67         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       1.00      0.50      0.67         2
       6076       1.00      0.50      0.67         2
       6082       1.00      1.00      1.00         2
       6085       1.00      0.50      0.67         2
       6087       1.00      0.50      0.67         2
       6090       1.00      1.00      1.00         2
       6097       1.00      0.50      0.67         2
       6101       0.50      0.50      0.50         2
       6103       0.50      0.50      0.50         2
       6108       0.67      1.00      0.80         2
       6112       0.67      1.00      0.80         2
       6123       1.00      1.00      1.00         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.67      1.00      0.80         2
       6159       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       1.00      1.00      1.00         2
       6182       1.00      0.50      0.67         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       1.00      0.50      0.67         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       0.50      1.00      0.67         2

avg / total       0.71      0.68      0.67       200


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.1}

0-1 loss 0.205
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       0.67      1.00      0.80         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.12      0.50      0.20         2
         61       0.40      1.00      0.57         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.40      1.00      0.57         2
        130       0.00      0.00      0.00         2
        135       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        138       0.00      0.00      0.00         2
        139       1.00      0.50      0.67         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.25      0.50      0.33         2
        202       0.00      0.00      0.00         2
        203       0.33      1.00      0.50         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.17      0.50      0.25         2
        245       0.67      1.00      0.80         2
        246       0.33      0.50      0.40         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.25      1.00      0.40         2
        314       0.50      0.50      0.50         2
        320       0.25      0.50      0.33         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.50      1.00      0.67         2
       3008       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       3019       0.40      1.00      0.57         2
       3032       0.00      0.00      0.00         2
       3042       0.50      0.50      0.50         2
       3064       0.00      0.00      0.00         2
       3091       1.00      0.50      0.67         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       0.50      0.50      0.50         2
       6005       0.33      0.50      0.40         2
       6006       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.25      0.50      0.33         2
       6013       0.00      0.00      0.00         2
       6015       0.33      0.50      0.40         2
       6016       0.00      0.00      0.00         2
       6023       1.00      1.00      1.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       0.00      0.00      0.00         2
       6066       0.67      1.00      0.80         2
       6076       0.00      0.00      0.00         2
       6082       0.40      1.00      0.57         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.00      0.00      0.00         2
       6129       0.00      0.00      0.00         2
       6131       0.00      0.00      0.00         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.33      0.50      0.40         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.33      0.50      0.40         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.33      0.50      0.40         2
       6211       0.00      0.00      0.00         2

avg / total       0.15      0.20      0.16       200


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'C': 128, 'gamma': 0.0001}

0-1 loss 0.7
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       0.67      1.00      0.80         2
         29       0.25      1.00      0.40         2
         61       1.00      1.00      1.00         2
         75       0.33      0.50      0.40         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        135       1.00      1.00      1.00         2
        137       1.00      1.00      1.00         2
        138       1.00      0.50      0.67         2
        139       1.00      1.00      1.00         2
        169       1.00      1.00      1.00         2
        179       1.00      0.50      0.67         2
        188       0.67      1.00      0.80         2
        194       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.50      0.50      0.50         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       0.67      1.00      0.80         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.20      0.50      0.29         2
        263       0.00      0.00      0.00         2
        276       1.00      0.50      0.67         2
        280       1.00      1.00      1.00         2
        294       0.50      1.00      0.67         2
        302       1.00      1.00      1.00         2
        303       0.67      1.00      0.80         2
        311       0.17      0.50      0.25         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       1.00      0.50      0.67         2
        358       1.00      0.50      0.67         2
        388       1.00      0.50      0.67         2
        415       0.67      1.00      0.80         2
       3001       1.00      1.00      1.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       0.67      1.00      0.80         2
       3042       1.00      1.00      1.00         2
       3064       1.00      1.00      1.00         2
       3091       1.00      0.50      0.67         2
       3092       0.67      1.00      0.80         2
       3125       1.00      1.00      1.00         2
       3147       1.00      1.00      1.00         2
       6005       1.00      0.50      0.67         2
       6006       1.00      0.50      0.67         2
       6008       0.20      0.50      0.29         2
       6010       1.00      1.00      1.00         2
       6011       0.00      0.00      0.00         2
       6012       0.20      0.50      0.29         2
       6013       0.00      0.00      0.00         2
       6015       1.00      1.00      1.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       1.00      0.50      0.67         2
       6076       1.00      1.00      1.00         2
       6082       1.00      1.00      1.00         2
       6085       0.50      0.50      0.50         2
       6087       0.33      0.50      0.40         2
       6090       1.00      1.00      1.00         2
       6097       0.50      0.50      0.50         2
       6101       0.50      0.50      0.50         2
       6103       0.50      0.50      0.50         2
       6108       0.67      1.00      0.80         2
       6112       1.00      1.00      1.00         2
       6123       1.00      1.00      1.00         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.50      1.00      0.67         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.50      0.50      0.50         2
       6182       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      1.00      1.00         2
       6210       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.71      0.70      0.68       200


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'C': 32, 'gamma': 0.001}

0-1 loss 0.505
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       0.29      1.00      0.44         2
         19       0.33      0.50      0.40         2
         20       0.50      0.50      0.50         2
         24       0.50      0.50      0.50         2
         29       0.25      1.00      0.40         2
         61       1.00      1.00      1.00         2
         75       0.67      1.00      0.80         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.25      0.50      0.33         2
        130       0.00      0.00      0.00         2
        135       0.50      0.50      0.50         2
        137       0.50      0.50      0.50         2
        138       0.00      0.00      0.00         2
        139       0.50      0.50      0.50         2
        169       0.00      0.00      0.00         2
        179       0.50      0.50      0.50         2
        188       0.33      0.50      0.40         2
        194       1.00      0.50      0.67         2
        202       0.33      0.50      0.40         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       1.00      0.50      0.67         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.25      0.50      0.33         2
        253       0.00      0.00      0.00         2
        254       1.00      0.50      0.67         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       0.67      1.00      0.80         2
        294       0.67      1.00      0.80         2
        302       1.00      1.00      1.00         2
        303       0.50      0.50      0.50         2
        311       0.33      1.00      0.50         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       0.67      1.00      0.80         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       1.00      0.50      0.67         2
        415       0.33      0.50      0.40         2
       3001       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      0.50      0.67         2
       3019       1.00      1.00      1.00         2
       3032       1.00      1.00      1.00         2
       3042       1.00      0.50      0.67         2
       3064       1.00      1.00      1.00         2
       3091       1.00      0.50      0.67         2
       3092       0.67      1.00      0.80         2
       3125       0.00      0.00      0.00         2
       3147       1.00      1.00      1.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       0.50      0.50      0.50         2
       6011       1.00      0.50      0.67         2
       6012       0.25      0.50      0.33         2
       6013       0.00      0.00      0.00         2
       6015       1.00      1.00      1.00         2
       6016       1.00      0.50      0.67         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       0.50      0.50      0.50         2
       6066       1.00      1.00      1.00         2
       6076       0.67      1.00      0.80         2
       6082       1.00      1.00      1.00         2
       6085       0.33      0.50      0.40         2
       6087       1.00      0.50      0.67         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.50      0.50      0.50         2
       6103       0.33      0.50      0.40         2
       6108       0.50      0.50      0.50         2
       6112       0.00      0.00      0.00         2
       6123       1.00      1.00      1.00         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.20      0.50      0.29         2
       6136       0.50      0.50      0.50         2
       6145       1.00      0.50      0.67         2
       6159       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6180       0.33      0.50      0.40         2
       6181       0.00      0.00      0.00         2
       6182       0.40      1.00      0.57         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6210       0.33      0.50      0.40         2
       6211       0.50      0.50      0.50         2

avg / total       0.53      0.51      0.49       200


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'C': 128, 'gamma': 0.0001}

0-1 loss 0.675
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       1.00      1.00      1.00         2
         29       0.29      1.00      0.44         2
         61       1.00      1.00      1.00         2
         75       0.14      0.50      0.22         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      1.00      1.00         2
        135       0.67      1.00      0.80         2
        137       1.00      1.00      1.00         2
        138       1.00      1.00      1.00         2
        139       1.00      1.00      1.00         2
        169       1.00      1.00      1.00         2
        179       0.50      0.50      0.50         2
        188       0.67      1.00      0.80         2
        194       1.00      1.00      1.00         2
        202       1.00      0.50      0.67         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.50      0.50      0.50         2
        234       0.33      0.50      0.40         2
        244       0.50      0.50      0.50         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       1.00      1.00      1.00         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       1.00      1.00      1.00         2
        294       0.50      1.00      0.67         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.08      0.50      0.14         2
        312       1.00      1.00      1.00         2
        314       0.67      1.00      0.80         2
        320       1.00      1.00      1.00         2
        322       0.50      0.50      0.50         2
        358       1.00      0.50      0.67         2
        388       0.67      1.00      0.80         2
        415       1.00      1.00      1.00         2
       3001       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       1.00      1.00      1.00         2
       3042       1.00      1.00      1.00         2
       3064       1.00      1.00      1.00         2
       3091       1.00      0.50      0.67         2
       3092       0.50      0.50      0.50         2
       3125       1.00      1.00      1.00         2
       3147       0.67      1.00      0.80         2
       6005       0.50      0.50      0.50         2
       6006       1.00      0.50      0.67         2
       6008       0.22      1.00      0.36         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       1.00      0.50      0.67         2
       6013       0.00      0.00      0.00         2
       6015       1.00      1.00      1.00         2
       6016       1.00      0.50      0.67         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       1.00      0.50      0.67         2
       6076       1.00      1.00      1.00         2
       6082       1.00      0.50      0.67         2
       6085       1.00      0.50      0.67         2
       6087       1.00      0.50      0.67         2
       6090       0.67      1.00      0.80         2
       6097       0.50      0.50      0.50         2
       6101       0.50      0.50      0.50         2
       6103       0.50      0.50      0.50         2
       6108       0.67      1.00      0.80         2
       6112       0.67      1.00      0.80         2
       6123       1.00      1.00      1.00         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.50      1.00      0.67         2
       6159       1.00      0.50      0.67         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.67      1.00      0.80         2
       6182       1.00      0.50      0.67         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.70      0.68      0.66       200


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'C': 64, 'gamma': 0.0001}

0-1 loss 0.695
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       1.00      1.00      1.00         2
         29       0.25      1.00      0.40         2
         61       1.00      1.00      1.00         2
         75       0.25      0.50      0.33         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        135       1.00      1.00      1.00         2
        137       1.00      1.00      1.00         2
        138       1.00      0.50      0.67         2
        139       1.00      1.00      1.00         2
        169       1.00      1.00      1.00         2
        179       1.00      0.50      0.67         2
        188       0.67      1.00      0.80         2
        194       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.50      0.50      0.50         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       0.67      1.00      0.80         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.25      0.50      0.33         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       1.00      1.00      1.00         2
        294       0.67      1.00      0.80         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.25      1.00      0.40         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       1.00      0.50      0.67         2
        358       1.00      0.50      0.67         2
        388       1.00      0.50      0.67         2
        415       0.67      1.00      0.80         2
       3001       1.00      1.00      1.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       1.00      1.00      1.00         2
       3042       1.00      1.00      1.00         2
       3064       0.50      1.00      0.67         2
       3091       1.00      0.50      0.67         2
       3092       0.50      0.50      0.50         2
       3125       1.00      1.00      1.00         2
       3147       0.67      1.00      0.80         2
       6005       1.00      0.50      0.67         2
       6006       1.00      0.50      0.67         2
       6008       0.17      0.50      0.25         2
       6010       1.00      1.00      1.00         2
       6011       0.00      0.00      0.00         2
       6012       0.25      0.50      0.33         2
       6013       0.00      0.00      0.00         2
       6015       1.00      1.00      1.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       1.00      0.50      0.67         2
       6076       1.00      1.00      1.00         2
       6082       1.00      1.00      1.00         2
       6085       0.50      0.50      0.50         2
       6087       0.33      0.50      0.40         2
       6090       1.00      1.00      1.00         2
       6097       0.50      0.50      0.50         2
       6101       0.67      1.00      0.80         2
       6103       0.50      0.50      0.50         2
       6108       0.67      1.00      0.80         2
       6112       1.00      1.00      1.00         2
       6123       1.00      1.00      1.00         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.50      1.00      0.67         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.50      0.50      0.50         2
       6182       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       0.33      0.50      0.40         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.70      0.69      0.68       200


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'C': 128, 'gamma': 0.0001}

0-1 loss 0.76
Baseline 0.01

Detailed classification report:


             precision    recall  f1-score   support

          3       0.67      1.00      0.80         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       1.00      0.50      0.67         2
         29       0.33      1.00      0.50         2
         61       0.67      1.00      0.80         2
         75       1.00      1.00      1.00         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        135       1.00      1.00      1.00         2
        137       1.00      1.00      1.00         2
        138       1.00      0.50      0.67         2
        139       1.00      1.00      1.00         2
        169       0.33      0.50      0.40         2
        179       0.67      1.00      0.80         2
        188       0.67      1.00      0.80         2
        194       1.00      1.00      1.00         2
        202       1.00      0.50      0.67         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       1.00      0.50      0.67         2
        234       0.00      0.00      0.00         2
        244       1.00      0.50      0.67         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       1.00      0.50      0.67         2
        252       0.67      1.00      0.80         2
        253       0.67      1.00      0.80         2
        254       0.00      0.00      0.00         2
        255       1.00      0.50      0.67         2
        258       0.25      0.50      0.33         2
        263       1.00      0.50      0.67         2
        276       1.00      1.00      1.00         2
        280       1.00      1.00      1.00         2
        294       0.50      1.00      0.67         2
        302       0.67      1.00      0.80         2
        303       1.00      1.00      1.00         2
        311       0.29      1.00      0.44         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       1.00      1.00      1.00         2
        358       1.00      0.50      0.67         2
        388       1.00      0.50      0.67         2
        415       0.67      1.00      0.80         2
       3001       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       1.00      1.00      1.00         2
       3042       1.00      1.00      1.00         2
       3064       0.67      1.00      0.80         2
       3091       1.00      1.00      1.00         2
       3092       1.00      1.00      1.00         2
       3125       1.00      1.00      1.00         2
       3147       0.67      1.00      0.80         2
       6005       0.50      0.50      0.50         2
       6006       1.00      0.50      0.67         2
       6008       0.20      0.50      0.29         2
       6010       1.00      0.50      0.67         2
       6011       1.00      1.00      1.00         2
       6012       0.25      0.50      0.33         2
       6013       1.00      0.50      0.67         2
       6015       1.00      1.00      1.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       1.00      1.00      1.00         2
       6066       1.00      1.00      1.00         2
       6076       1.00      1.00      1.00         2
       6082       1.00      1.00      1.00         2
       6085       0.50      0.50      0.50         2
       6087       1.00      0.50      0.67         2
       6090       0.67      1.00      0.80         2
       6097       1.00      0.50      0.67         2
       6101       1.00      0.50      0.67         2
       6103       1.00      0.50      0.67         2
       6108       1.00      1.00      1.00         2
       6112       1.00      1.00      1.00         2
       6123       1.00      1.00      1.00         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.50      1.00      0.67         2
       6159       0.67      1.00      0.80         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6181       0.50      0.50      0.50         2
       6182       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.80      0.76      0.75       200


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 8}

0-1 loss 0.11
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [   3   19  129  188  206  244  245  246  249  255  263  280  314  388 3019
 3032 3042 3064 3092 3125 6006 6010 6011 6015 6016 6023 6053 6076 6087 6097
 6103 6108 6112 6124 6129 6145 6181 6182 6200 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [   3   19   20   29  129  130  135  137  138  139  169  179  188  194  202
  206  234  244  245  246  249  253  254  255  258  263  276  280  294  302
  303  311  312  314  358  388 3001 3006 3019 3032 3042 3064 3091 3092 3125
 6005 6006 6010 6011 6012 6013 6015 6016 6023 6033 6037 6053 6066 6076 6085
 6087 6090 6097 6101 6103 6108 6112 6123 6124 6129 6136 6145 6171 6180 6181
 6182 6200 6203 6204 6210 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.00      0.00      0.00         2
         19       0.00      0.00      0.00         2
         20       0.00      0.00      0.00         2
         24       0.33      0.50      0.40         2
         29       0.00      0.00      0.00         2
         61       0.50      0.50      0.50         2
         75       0.67      1.00      0.80         2
         82       0.20      0.50      0.29         2
        114       0.33      0.50      0.40         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        203       0.40      1.00      0.57         2
        206       0.00      0.00      0.00         2
        228       0.25      0.50      0.33         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.00      0.00      0.00         2
        246       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.12      0.50      0.20         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       0.50      0.50      0.50         2
        322       0.50      0.50      0.50         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       1.00      0.50      0.67         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.50      1.00      0.67         2
       3013       0.50      0.50      0.50         2
       3019       0.00      0.00      0.00         2
       3032       0.00      0.00      0.00         2
       3042       0.00      0.00      0.00         2
       3064       0.00      0.00      0.00         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       0.11      0.50      0.18         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       0.25      0.50      0.33         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.50      0.50      0.50         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.00      0.00      0.00         2
       6129       0.00      0.00      0.00         2
       6131       0.25      0.50      0.33         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.50      0.50      0.50         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.33      0.50      0.40         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.08      0.11      0.09       200


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 1}

0-1 loss 0.22
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 129  203  246  253  358 3001 3091 3125 6006 6010 6013 6076 6090 6123]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   20   24   29   61   75   82  114  129  138  139  169  179  202  203
  206  228  244  246  253  254  255  258  263  276  303  311  312  358  388
  415 3001 3006 3013 3091 3125 6005 6006 6010 6012 6013 6016 6023 6033 6053
 6066 6076 6085 6087 6090 6101 6112 6123 6129 6136 6145 6180 6181 6182 6200
 6203 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.29      1.00      0.44         2
         19       0.00      0.00      0.00         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       0.00      0.00      0.00         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       0.33      0.50      0.40         2
        135       0.50      0.50      0.50         2
        137       0.50      0.50      0.50         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       1.00      0.50      0.67         2
        194       0.50      1.00      0.67         2
        202       0.00      0.00      0.00         2
        203       0.00      0.00      0.00         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.33      0.50      0.40         2
        244       0.00      0.00      0.00         2
        245       0.20      0.50      0.29         2
        246       0.00      0.00      0.00         2
        249       0.50      0.50      0.50         2
        252       0.40      1.00      0.57         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.50      0.50      0.50         2
        302       1.00      0.50      0.67         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       0.50      0.50      0.50         2
        320       1.00      0.50      0.67         2
        322       0.50      1.00      0.67         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      0.50      0.67         2
       3013       0.00      0.00      0.00         2
       3019       1.00      0.50      0.67         2
       3032       1.00      0.50      0.67         2
       3042       0.33      0.50      0.40         2
       3064       0.50      0.50      0.50         2
       3091       0.00      0.00      0.00         2
       3092       0.50      0.50      0.50         2
       3125       0.00      0.00      0.00         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       0.33      0.50      0.40         2
       6010       0.00      0.00      0.00         2
       6011       0.25      0.50      0.33         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.33      0.50      0.40         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.50      0.50      0.50         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.25      0.50      0.33         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.25      0.50      0.33         2
       6101       0.00      0.00      0.00         2
       6103       0.33      0.50      0.40         2
       6108       0.50      0.50      0.50         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.50      0.50      0.50         2
       6129       0.00      0.00      0.00         2
       6131       0.25      0.50      0.33         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.40      1.00      0.57         2
       6171       1.00      0.50      0.67         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.67      1.00      0.80         2
       6210       0.50      0.50      0.50         2
       6211       0.00      0.00      0.00         2

avg / total       0.21      0.22      0.20       200


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'max_depth': 8, 'min_samples_leaf': 1}

0-1 loss 0.065
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  20  130  135  139  169  206  228  244  249  252  253  254  255  263  276
  311  312  314  388  415 3013 3042 3064 3092 3125 6005 6006 6008 6010 6011
 6012 6013 6016 6023 6033 6053 6076 6085 6087 6090 6103 6108 6112 6123 6124
 6129 6136 6171 6180 6181 6182 6200 6203 6204 6211]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [   3   20   24   29   75   82  114  129  130  135  138  139  169  179  188
  202  206  228  234  244  246  249  252  253  254  255  258  263  276  294
  302  303  311  312  314  320  322  358  388  415 3001 3006 3008 3013 3019
 3032 3042 3064 3092 3125 6005 6006 6008 6010 6011 6012 6013 6015 6016 6023
 6033 6037 6053 6076 6082 6085 6087 6090 6097 6101 6103 6108 6112 6123 6124
 6129 6136 6159 6171 6180 6181 6182 6200 6203 6204 6210 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.00      0.00      0.00         2
         19       0.33      0.50      0.40         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       0.20      0.50      0.29         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       0.00      0.00      0.00         2
        137       0.07      0.50      0.12         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.25      0.50      0.33         2
        202       0.00      0.00      0.00         2
        203       0.09      0.50      0.15         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.20      0.50      0.29         2
        246       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.25      0.50      0.33         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       3019       0.00      0.00      0.00         2
       3032       0.00      0.00      0.00         2
       3042       0.00      0.00      0.00         2
       3064       0.00      0.00      0.00         2
       3091       1.00      0.50      0.67         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.33      0.50      0.40         2
       6076       0.00      0.00      0.00         2
       6082       0.00      0.00      0.00         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.00      0.00      0.00         2
       6129       0.00      0.00      0.00         2
       6131       0.10      0.50      0.17         2
       6136       0.00      0.00      0.00         2
       6145       0.05      0.50      0.10         2
       6159       0.00      0.00      0.00         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.50      0.50      0.50         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.04      0.07      0.04       200


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 1}

0-1 loss 0.24
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19   82  203  206  320 3001 3006 3091 6012 6033 6053 6076 6085 6112 6123
 6129 6136 6197 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   20   24   61   75   82  129  138  139  169  179  202  203  206  228
  234  244  253  255  258  263  276  303  311  312  320  358  388 3001 3006
 3013 3091 3092 6005 6010 6012 6013 6015 6016 6023 6033 6037 6053 6076 6085
 6090 6101 6112 6123 6129 6136 6145 6159 6180 6181 6182 6197 6200 6203 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.25      0.50      0.33         2
         19       0.00      0.00      0.00         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.25      0.50      0.33         2
         61       0.00      0.00      0.00         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.50      0.50      0.50         2
        129       0.00      0.00      0.00         2
        130       0.33      0.50      0.40         2
        135       0.33      0.50      0.40         2
        137       0.50      0.50      0.50         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.25      0.50      0.33         2
        194       0.50      1.00      0.67         2
        202       0.00      0.00      0.00         2
        203       0.00      0.00      0.00         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.50      0.50      0.50         2
        249       0.25      0.50      0.33         2
        252       0.50      1.00      0.67         2
        253       0.00      0.00      0.00         2
        254       0.33      0.50      0.40         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.50      0.50      0.50         2
        294       0.33      0.50      0.40         2
        302       1.00      1.00      1.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       0.50      0.50      0.50         2
        320       0.00      0.00      0.00         2
        322       0.33      0.50      0.40         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.25      1.00      0.40         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.50      0.50      0.50         2
       3013       0.00      0.00      0.00         2
       3019       1.00      0.50      0.67         2
       3032       1.00      0.50      0.67         2
       3042       0.50      0.50      0.50         2
       3064       0.67      1.00      0.80         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       1.00      0.50      0.67         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       0.50      0.50      0.50         2
       6008       0.50      0.50      0.50         2
       6010       0.00      0.00      0.00         2
       6011       0.33      0.50      0.40         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.50      0.50      0.50         2
       6076       0.00      0.00      0.00         2
       6082       0.50      0.50      0.50         2
       6085       0.00      0.00      0.00         2
       6087       0.33      0.50      0.40         2
       6090       0.00      0.00      0.00         2
       6097       0.50      0.50      0.50         2
       6101       0.00      0.00      0.00         2
       6103       0.50      0.50      0.50         2
       6108       0.33      0.50      0.40         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.33      0.50      0.40         2
       6129       0.00      0.00      0.00         2
       6131       0.33      1.00      0.50         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.20      0.50      0.29         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.00      0.00      0.00         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.67      1.00      0.80         2
       6210       1.00      0.50      0.67         2
       6211       0.00      0.00      0.00         2

avg / total       0.21      0.24      0.21       200


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 1}

0-1 loss 0.12
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [   3   19  254  263  276  303  415 3032 3064 3091 6016 6066 6090 6101 6108
 6145]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [   3   19   20   24   29   82  129  130  135  137  138  169  179  188  202
  206  244  249  252  253  254  255  258  263  276  280  294  302  303  311
  312  314  322  358  388  415 3001 3006 3008 3019 3032 3042 3064 3091 3092
 3125 6005 6006 6010 6011 6012 6013 6015 6016 6023 6033 6037 6053 6066 6076
 6087 6090 6097 6101 6103 6108 6112 6123 6136 6145 6171 6180 6181 6182 6197
 6200 6203 6204 6210]. 
  average=None)
             precision    recall  f1-score   support

          3       0.00      0.00      0.00         2
         19       0.00      0.00      0.00         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       1.00      0.50      0.67         2
         75       0.67      1.00      0.80         2
         82       0.00      0.00      0.00         2
        114       0.50      0.50      0.50         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        138       0.00      0.00      0.00         2
        139       0.20      0.50      0.29         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.50      0.50      0.50         2
        202       0.00      0.00      0.00         2
        203       0.67      1.00      0.80         2
        206       0.00      0.00      0.00         2
        228       1.00      0.50      0.67         2
        234       0.33      0.50      0.40         2
        244       0.00      0.00      0.00         2
        245       0.50      0.50      0.50         2
        246       0.33      0.50      0.40         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       0.20      0.50      0.29         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.00      0.00      0.00         2
       3013       0.50      0.50      0.50         2
       3019       0.00      0.00      0.00         2
       3032       0.00      0.00      0.00         2
       3042       0.00      0.00      0.00         2
       3064       0.00      0.00      0.00         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       0.25      0.50      0.33         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       0.12      0.50      0.20         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.33      0.50      0.40         2
       6085       0.17      0.50      0.25         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.50      0.50      0.50         2
       6129       0.50      0.50      0.50         2
       6131       0.33      0.50      0.40         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.50      1.00      0.67         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.00      0.00      0.00         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.00      0.00      0.00         2
       6211       0.50      0.50      0.50         2

avg / total       0.10      0.12      0.10       200


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'max_depth': None, 'min_samples_leaf': 2}

0-1 loss 0.21
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 322  358 3001 3091 6011 6016 6033 6085 6101 6180 6181 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  20   24   29   61   75   82  114  129  138  169  179  202  203  206  228
  234  244  254  263  276  294  303  311  312  320  322  358  388  415 3001
 3006 3013 3091 3092 3125 6005 6010 6011 6012 6015 6016 6023 6033 6037 6053
 6066 6076 6085 6087 6097 6101 6112 6123 6129 6136 6180 6181 6182 6200 6203
 6204 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.33      1.00      0.50         2
         19       0.50      0.50      0.50         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       0.00      0.00      0.00         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       1.00      0.50      0.67         2
        135       0.29      1.00      0.44         2
        137       0.33      0.50      0.40         2
        138       0.00      0.00      0.00         2
        139       0.50      0.50      0.50         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       1.00      0.50      0.67         2
        194       0.50      0.50      0.50         2
        202       0.00      0.00      0.00         2
        203       0.00      0.00      0.00         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.33      0.50      0.40         2
        246       0.50      0.50      0.50         2
        249       0.50      0.50      0.50         2
        252       0.50      1.00      0.67         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.20      0.50      0.29         2
        258       0.50      0.50      0.50         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.00      0.00      0.00         2
        302       1.00      1.00      1.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       1.00      0.50      0.67         2
        320       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      0.50      0.67         2
       3013       0.00      0.00      0.00         2
       3019       1.00      0.50      0.67         2
       3032       0.50      0.50      0.50         2
       3042       0.50      0.50      0.50         2
       3064       0.50      0.50      0.50         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       1.00      0.50      0.67         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.33      0.50      0.40         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       1.00      0.50      0.67         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.33      0.50      0.40         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.20      0.50      0.29         2
       6108       0.50      0.50      0.50         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.20      0.50      0.29         2
       6136       0.00      0.00      0.00         2
       6145       1.00      0.50      0.67         2
       6159       0.50      0.50      0.50         2
       6171       0.33      0.50      0.40         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.33      0.50      0.40         2
       6211       0.00      0.00      0.00         2

avg / total       0.23      0.21      0.20       200


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'max_depth': 64, 'min_samples_leaf': 4}

0-1 loss 0.205
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  20  129  206  312 3001 3091 3125 6005 6006 6010 6082 6112 6124 6136 6145
 6180 6181 6182 6200 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  20   24   61   82  114  129  130  169  179  188  202  203  206  228  234
  244  249  254  255  258  263  276  302  303  311  312  320  358  388  415
 3001 3006 3013 3091 3092 3125 6005 6006 6010 6011 6012 6013 6015 6016 6023
 6033 6053 6066 6076 6082 6087 6101 6112 6123 6124 6136 6145 6159 6180 6181
 6182 6200 6203 6210 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.11      0.50      0.18         2
         19       0.25      0.50      0.33         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.50      0.50      0.50         2
         61       0.00      0.00      0.00         2
         75       0.25      0.50      0.33         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       1.00      1.00      1.00         2
        137       0.25      0.50      0.33         2
        138       1.00      0.50      0.67         2
        139       0.25      1.00      0.40         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.33      0.50      0.40         2
        202       0.00      0.00      0.00         2
        203       0.00      0.00      0.00         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.50      0.50      0.50         2
        249       0.00      0.00      0.00         2
        252       0.50      1.00      0.67         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.50      0.50      0.50         2
        294       0.50      0.50      0.50         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.00      0.00      0.00         2
        314       1.00      0.50      0.67         2
        320       0.00      0.00      0.00         2
        322       0.67      1.00      0.80         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      0.50      0.67         2
       3013       0.00      0.00      0.00         2
       3019       1.00      0.50      0.67         2
       3032       0.50      0.50      0.50         2
       3042       1.00      0.50      0.67         2
       3064       0.67      1.00      0.80         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       1.00      0.50      0.67         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.50      0.50      0.50         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.00      0.00      0.00         2
       6085       0.25      0.50      0.33         2
       6087       0.00      0.00      0.00         2
       6090       0.50      0.50      0.50         2
       6097       1.00      0.50      0.67         2
       6101       0.00      0.00      0.00         2
       6103       1.00      0.50      0.67         2
       6108       1.00      0.50      0.67         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.00      0.00      0.00         2
       6129       1.00      0.50      0.67         2
       6131       0.33      0.50      0.40         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.50      0.50      0.50         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.25      0.50      0.33         2
       6210       0.00      0.00      0.00         2
       6211       0.00      0.00      0.00         2

avg / total       0.23      0.20      0.20       200


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'max_depth': 32, 'min_samples_leaf': 2}

0-1 loss 0.195
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  24  228  244  322 3001 6012 6013 6016 6023 6033 6082 6097 6112 6124 6129
 6136 6145 6181 6200 6203]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [   3   20   24   29   75   82  114  130  138  139  169  202  203  206  228
  234  244  249  253  254  255  263  276  294  302  312  320  322  358  388
  415 3001 3006 3013 3091 6005 6006 6010 6012 6013 6015 6016 6023 6033 6037
 6053 6066 6076 6082 6085 6087 6090 6097 6101 6112 6124 6129 6136 6145 6159
 6180 6181 6200 6203 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.00      0.00      0.00         2
         19       0.50      0.50      0.50         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.00      0.00      0.00         2
         61       0.33      0.50      0.40         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.33      0.50      0.40         2
        130       0.00      0.00      0.00         2
        135       0.25      0.50      0.33         2
        137       1.00      0.50      0.67         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.14      0.50      0.22         2
        188       0.50      0.50      0.50         2
        194       0.33      0.50      0.40         2
        202       0.00      0.00      0.00         2
        203       0.00      0.00      0.00         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      0.50      0.67         2
        246       0.50      0.50      0.50         2
        249       0.00      0.00      0.00         2
        252       0.33      1.00      0.50         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.20      0.50      0.29         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       1.00      0.50      0.67         2
        311       0.25      1.00      0.40         2
        312       0.00      0.00      0.00         2
        314       1.00      0.50      0.67         2
        320       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       1.00      0.50      0.67         2
       3013       0.00      0.00      0.00         2
       3019       1.00      0.50      0.67         2
       3032       0.33      0.50      0.40         2
       3042       0.50      0.50      0.50         2
       3064       0.67      1.00      0.80         2
       3091       0.00      0.00      0.00         2
       3092       0.50      0.50      0.50         2
       3125       1.00      0.50      0.67         2
       3147       1.00      0.50      0.67         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       1.00      0.50      0.67         2
       6010       0.00      0.00      0.00         2
       6011       0.25      0.50      0.33         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.00      0.00      0.00         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       1.00      0.50      0.67         2
       6108       1.00      0.50      0.67         2
       6112       0.00      0.00      0.00         2
       6123       0.20      0.50      0.29         2
       6124       0.00      0.00      0.00         2
       6129       0.00      0.00      0.00         2
       6131       0.20      0.50      0.29         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.50      0.50      0.50         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.50      0.50      0.50         2
       6197       0.50      0.50      0.50         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.67      1.00      0.80         2
       6210       0.33      0.50      0.40         2
       6211       0.00      0.00      0.00         2

avg / total       0.20      0.20      0.18       200


---------------------------------------------
F1 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.29
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  20   61  129  139  169  179  228  254  255  263  276  312  314 3019 3091
 6011 6012 6013 6015 6023 6037 6053 6112 6129 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  20   61   82  129  130  137  138  139  169  179  206  228  234  244  249
  252  253  254  255  258  263  276  312  314  322  415 3001 3019 3064 3091
 3147 6005 6008 6010 6011 6012 6013 6015 6023 6033 6037 6053 6066 6087 6090
 6103 6108 6112 6129 6159 6181 6200 6203 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.33      0.50      0.40         2
         19       0.50      0.50      0.50         2
         20       0.00      0.00      0.00         2
         24       0.20      0.50      0.29         2
         29       0.12      0.50      0.20         2
         61       0.00      0.00      0.00         2
         75       0.25      0.50      0.33         2
         82       0.00      0.00      0.00         2
        114       1.00      0.50      0.67         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       0.33      0.50      0.40         2
        137       0.00      0.00      0.00         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.50      0.50      0.50         2
        194       0.50      0.50      0.50         2
        202       0.14      0.50      0.22         2
        203       1.00      0.50      0.67         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.67      1.00      0.80         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.25      0.50      0.33         2
        294       0.25      0.50      0.33         2
        302       0.50      0.50      0.50         2
        303       0.50      0.50      0.50         2
        311       0.40      1.00      0.57         2
        312       0.00      0.00      0.00         2
        314       0.00      0.00      0.00         2
        320       1.00      0.50      0.67         2
        322       0.00      0.00      0.00         2
        358       0.33      0.50      0.40         2
        388       0.50      0.50      0.50         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.33      0.50      0.40         2
       3008       0.67      1.00      0.80         2
       3013       0.50      1.00      0.67         2
       3019       0.00      0.00      0.00         2
       3032       1.00      0.50      0.67         2
       3042       1.00      0.50      0.67         2
       3064       0.00      0.00      0.00         2
       3091       0.00      0.00      0.00         2
       3092       0.33      1.00      0.50         2
       3125       0.50      0.50      0.50         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.67      1.00      0.80         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.25      0.50      0.33         2
       6082       0.50      0.50      0.50         2
       6085       0.50      1.00      0.67         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.50      0.50      0.50         2
       6101       1.00      0.50      0.67         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.50      1.00      0.67         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       1.00      0.50      0.67         2
       6145       0.50      0.50      0.50         2
       6159       0.00      0.00      0.00         2
       6171       0.40      1.00      0.57         2
       6180       0.25      0.50      0.33         2
       6181       0.00      0.00      0.00         2
       6182       0.50      1.00      0.67         2
       6197       0.50      0.50      0.50         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.50      0.50      0.50         2
       6210       0.50      0.50      0.50         2
       6211       0.00      0.00      0.00         2

avg / total       0.25      0.29      0.25       200


---------------------------------------------
F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.43
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19   82  114  139  179  188  202  206  254  255  263  276  322 3001 3042
 3064 3147 6011 6015 6037 6066 6076 6085 6087 6103 6159 6180 6197 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19   82  114  139  179  188  202  206  234  244  253  254  255  258  263
  276  322  415 3001 3042 3064 3091 3147 6005 6011 6013 6015 6023 6033 6037
 6066 6076 6085 6087 6097 6103 6129 6136 6159 6180 6197 6200]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       0.00      0.00      0.00         2
         20       1.00      0.50      0.67         2
         24       1.00      0.50      0.67         2
         29       0.12      1.00      0.21         2
         61       0.50      1.00      0.67         2
         75       0.50      0.50      0.50         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.50      0.50      0.50         2
        130       1.00      0.50      0.67         2
        135       0.50      0.50      0.50         2
        137       0.40      1.00      0.57         2
        138       0.50      0.50      0.50         2
        139       0.00      0.00      0.00         2
        169       0.33      1.00      0.50         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.67      1.00      0.80         2
        202       0.00      0.00      0.00         2
        203       0.67      1.00      0.80         2
        206       0.00      0.00      0.00         2
        228       0.50      0.50      0.50         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.67      1.00      0.80         2
        246       0.67      1.00      0.80         2
        249       0.50      0.50      0.50         2
        252       0.20      0.50      0.29         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       1.00      0.50      0.67         2
        294       0.67      1.00      0.80         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.05      0.50      0.08         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      0.50      0.67         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.33      0.50      0.40         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       0.67      1.00      0.80         2
       3032       1.00      1.00      1.00         2
       3042       0.00      0.00      0.00         2
       3064       0.00      0.00      0.00         2
       3091       0.00      0.00      0.00         2
       3092       0.25      0.50      0.33         2
       3125       1.00      0.50      0.67         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.20      0.50      0.29         2
       6010       0.50      0.50      0.50         2
       6011       0.00      0.00      0.00         2
       6012       0.20      0.50      0.29         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.50      0.50      0.50         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       1.00      1.00      1.00         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.50      0.50      0.50         2
       6097       0.00      0.00      0.00         2
       6101       1.00      0.50      0.67         2
       6103       0.00      0.00      0.00         2
       6108       1.00      1.00      1.00         2
       6112       1.00      0.50      0.67         2
       6123       0.67      1.00      0.80         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.00      0.00      0.00         2
       6145       0.50      1.00      0.67         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.50      0.50      0.50         2
       6182       0.20      0.50      0.29         2
       6197       0.00      0.00      0.00         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       0.50      0.50      0.50         2
       6210       1.00      1.00      1.00         2
       6211       0.67      1.00      0.80         2

avg / total       0.40      0.43      0.39       200


---------------------------------------------
F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 17}

0-1 loss 0.11
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 139  179  206  228  234  258  263  276  294  302  303  358 3008 3013 3042
 3091 3092 3125 6005 6010 6011 6013 6015 6016 6033 6037 6053 6076 6087 6090
 6103 6108 6124 6136 6145 6159 6171 6181 6182 6200 6203 6204]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [   3   20   24   75   82  114  130  135  137  138  139  169  179  188  194
  202  206  228  234  246  249  252  253  254  255  258  263  276  280  294
  302  303  311  320  322  358  388 3001 3006 3008 3013 3042 3064 3091 3092
 3125 6005 6006 6008 6010 6011 6012 6013 6015 6016 6033 6037 6053 6066 6076
 6085 6087 6090 6097 6101 6103 6108 6112 6123 6124 6129 6131 6136 6145 6159
 6171 6180 6181 6182 6197 6200 6203 6204 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.00      0.00      0.00         2
         19       0.33      1.00      0.50         2
         20       0.00      0.00      0.00         2
         24       0.00      0.00      0.00         2
         29       0.11      1.00      0.19         2
         61       0.22      1.00      0.36         2
         75       0.00      0.00      0.00         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.50      1.00      0.67         2
        130       0.00      0.00      0.00         2
        135       0.00      0.00      0.00         2
        137       0.00      0.00      0.00         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.00      0.00      0.00         2
        202       0.00      0.00      0.00         2
        203       0.22      1.00      0.36         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.33      0.50      0.40         2
        245       1.00      0.50      0.67         2
        246       0.00      0.00      0.00         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.00      0.00      0.00         2
        302       0.00      0.00      0.00         2
        303       0.00      0.00      0.00         2
        311       0.00      0.00      0.00         2
        312       0.67      1.00      0.80         2
        314       0.20      0.50      0.29         2
        320       0.00      0.00      0.00         2
        322       0.00      0.00      0.00         2
        358       0.00      0.00      0.00         2
        388       0.00      0.00      0.00         2
        415       0.20      0.50      0.29         2
       3001       0.00      0.00      0.00         2
       3006       0.00      0.00      0.00         2
       3008       0.00      0.00      0.00         2
       3013       0.00      0.00      0.00         2
       3019       0.14      0.50      0.22         2
       3032       1.00      0.50      0.67         2
       3042       0.00      0.00      0.00         2
       3064       0.00      0.00      0.00         2
       3091       0.00      0.00      0.00         2
       3092       0.00      0.00      0.00         2
       3125       0.00      0.00      0.00         2
       3147       0.33      0.50      0.40         2
       6005       0.00      0.00      0.00         2
       6006       0.00      0.00      0.00         2
       6008       0.00      0.00      0.00         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.00      0.00      0.00         2
       6023       0.50      0.50      0.50         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       0.25      0.50      0.33         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       0.00      0.00      0.00         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.00      0.00      0.00         2
       6124       0.00      0.00      0.00         2
       6129       0.00      0.00      0.00         2
       6131       0.00      0.00      0.00         2
       6136       0.00      0.00      0.00         2
       6145       0.00      0.00      0.00         2
       6159       0.00      0.00      0.00         2
       6171       0.00      0.00      0.00         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.00      0.00      0.00         2
       6197       0.00      0.00      0.00         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.00      0.00      0.00         2
       6210       0.33      0.50      0.40         2
       6211       0.00      0.00      0.00         2

avg / total       0.06      0.11      0.07       200


---------------------------------------------
F1+F2 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.45
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  82  114  139  206  263  276  322 3001 3042 3147 6005 6008 6011 6013 6037
 6053 6066 6090 6112 6129 6159 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  82  114  139  179  206  234  244  249  253  254  255  258  263  276  322
 3001 3042 3147 6005 6008 6011 6013 6015 6023 6037 6053 6066 6076 6090 6097
 6112 6129 6136 6159 6180 6200]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       1.00      0.50      0.67         2
         20       0.50      0.50      0.50         2
         24       1.00      0.50      0.67         2
         29       0.17      0.50      0.25         2
         61       0.40      1.00      0.57         2
         75       0.25      0.50      0.33         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.33      0.50      0.40         2
        130       1.00      1.00      1.00         2
        135       0.33      1.00      0.50         2
        137       0.40      1.00      0.57         2
        138       0.50      0.50      0.50         2
        139       0.00      0.00      0.00         2
        169       1.00      1.00      1.00         2
        179       0.00      0.00      0.00         2
        188       1.00      0.50      0.67         2
        194       0.50      1.00      0.67         2
        202       1.00      0.50      0.67         2
        203       0.67      1.00      0.80         2
        206       0.00      0.00      0.00         2
        228       0.33      0.50      0.40         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       0.17      0.50      0.25         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.33      0.50      0.40         2
        294       0.50      0.50      0.50         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.18      1.00      0.31         2
        312       0.50      0.50      0.50         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.33      0.50      0.40         2
        415       1.00      0.50      0.67         2
       3001       0.00      0.00      0.00         2
       3006       0.67      1.00      0.80         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       0.50      1.00      0.67         2
       3032       0.67      1.00      0.80         2
       3042       0.00      0.00      0.00         2
       3064       1.00      0.50      0.67         2
       3091       0.50      0.50      0.50         2
       3092       0.20      0.50      0.29         2
       3125       1.00      0.50      0.67         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       1.00      1.00      1.00         2
       6011       0.00      0.00      0.00         2
       6012       0.20      0.50      0.29         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.33      0.50      0.40         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       1.00      0.50      0.67         2
       6085       1.00      0.50      0.67         2
       6087       1.00      0.50      0.67         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       1.00      0.50      0.67         2
       6103       1.00      0.50      0.67         2
       6108       1.00      1.00      1.00         2
       6112       0.00      0.00      0.00         2
       6123       0.29      1.00      0.44         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.33      1.00      0.50         2
       6136       0.00      0.00      0.00         2
       6145       0.50      0.50      0.50         2
       6159       0.00      0.00      0.00         2
       6171       0.67      1.00      0.80         2
       6180       0.00      0.00      0.00         2
       6181       0.10      0.50      0.17         2
       6182       0.25      0.50      0.33         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       0.50      0.50      0.50         2
       6210       0.50      0.50      0.50         2
       6211       1.00      0.50      0.67         2

avg / total       0.44      0.45      0.41       200


---------------------------------------------
F1+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.32
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [ 139  169  179  206  228  253  254  255  263  276  312 3042 6008 6010 6011
 6012 6013 6015 6023 6037 6053 6066 6112 6129 6136 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  20   82  129  130  138  139  169  179  206  228  234  244  249  252  253
  254  255  258  263  276  280  312  322  415 3001 3042 6005 6008 6010 6011
 6012 6013 6015 6023 6033 6037 6053 6066 6087 6090 6103 6108 6112 6129 6136
 6180 6181 6200 6203 6211]. 
  average=None)
             precision    recall  f1-score   support

          3       0.33      0.50      0.40         2
         19       0.50      0.50      0.50         2
         20       0.00      0.00      0.00         2
         24       1.00      0.50      0.67         2
         29       0.11      0.50      0.18         2
         61       1.00      0.50      0.67         2
         75       0.25      0.50      0.33         2
         82       0.00      0.00      0.00         2
        114       1.00      0.50      0.67         2
        129       0.00      0.00      0.00         2
        130       0.00      0.00      0.00         2
        135       0.33      0.50      0.40         2
        137       0.33      0.50      0.40         2
        138       0.00      0.00      0.00         2
        139       0.00      0.00      0.00         2
        169       0.00      0.00      0.00         2
        179       0.00      0.00      0.00         2
        188       0.50      0.50      0.50         2
        194       0.33      0.50      0.40         2
        202       0.17      0.50      0.25         2
        203       0.50      0.50      0.50         2
        206       0.00      0.00      0.00         2
        228       0.00      0.00      0.00         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       0.67      1.00      0.80         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.00      0.00      0.00         2
        294       0.33      0.50      0.40         2
        302       0.33      0.50      0.40         2
        303       0.33      0.50      0.40         2
        311       0.33      0.50      0.40         2
        312       0.00      0.00      0.00         2
        314       1.00      0.50      0.67         2
        320       1.00      0.50      0.67         2
        322       0.00      0.00      0.00         2
        358       0.50      0.50      0.50         2
        388       0.50      0.50      0.50         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       1.00      0.50      0.67         2
       3008       0.67      1.00      0.80         2
       3013       0.50      0.50      0.50         2
       3019       0.67      1.00      0.80         2
       3032       1.00      1.00      1.00         2
       3042       0.00      0.00      0.00         2
       3064       0.50      0.50      0.50         2
       3091       1.00      0.50      0.67         2
       3092       0.50      1.00      0.67         2
       3125       0.50      0.50      0.50         2
       3147       1.00      1.00      1.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       0.00      0.00      0.00         2
       6011       0.00      0.00      0.00         2
       6012       0.00      0.00      0.00         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.50      1.00      0.67         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.40      1.00      0.57         2
       6082       0.50      0.50      0.50         2
       6085       0.50      1.00      0.67         2
       6087       0.00      0.00      0.00         2
       6090       0.00      0.00      0.00         2
       6097       0.50      0.50      0.50         2
       6101       0.50      0.50      0.50         2
       6103       0.00      0.00      0.00         2
       6108       0.00      0.00      0.00         2
       6112       0.00      0.00      0.00         2
       6123       0.50      1.00      0.67         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.50      1.00      0.67         2
       6136       0.00      0.00      0.00         2
       6145       0.50      0.50      0.50         2
       6159       1.00      0.50      0.67         2
       6171       0.33      1.00      0.50         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.50      1.00      0.67         2
       6197       0.50      0.50      0.50         2
       6200       0.00      0.00      0.00         2
       6203       0.00      0.00      0.00         2
       6204       0.50      0.50      0.50         2
       6210       0.50      0.50      0.50         2
       6211       0.00      0.00      0.00         2

avg / total       0.29      0.32      0.28       200


---------------------------------------------
F2+F3 features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.45
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19  179  188  202  206  254  263  276  322 3001 3042 3147 6011 6015 6033
 6037 6066 6076 6085 6087 6103 6159 6180 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19  179  188  202  206  234  244  249  252  253  254  255  258  263  276
  322  415 3001 3042 3147 6005 6011 6013 6015 6023 6033 6037 6066 6076 6085
 6087 6103 6129 6136 6159 6180 6200]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       0.00      0.00      0.00         2
         20       1.00      0.50      0.67         2
         24       1.00      0.50      0.67         2
         29       0.10      0.50      0.17         2
         61       0.40      1.00      0.57         2
         75       0.25      0.50      0.33         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.25      0.50      0.33         2
        130       1.00      0.50      0.67         2
        135       0.40      1.00      0.57         2
        137       0.67      1.00      0.80         2
        138       0.50      0.50      0.50         2
        139       1.00      0.50      0.67         2
        169       0.29      1.00      0.44         2
        179       0.00      0.00      0.00         2
        188       0.00      0.00      0.00         2
        194       0.67      1.00      0.80         2
        202       0.00      0.00      0.00         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.33      0.50      0.40         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.00      0.00      0.00         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       1.00      0.50      0.67         2
        294       1.00      0.50      0.67         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.08      0.50      0.13         2
        312       0.67      1.00      0.80         2
        314       0.67      1.00      0.80         2
        320       1.00      0.50      0.67         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.20      0.50      0.29         2
        415       0.00      0.00      0.00         2
       3001       0.00      0.00      0.00         2
       3006       0.67      1.00      0.80         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       0.50      1.00      0.67         2
       3032       1.00      1.00      1.00         2
       3042       0.00      0.00      0.00         2
       3064       1.00      0.50      0.67         2
       3091       0.50      0.50      0.50         2
       3092       0.33      0.50      0.40         2
       3125       1.00      0.50      0.67         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       0.50      0.50      0.50         2
       6008       0.20      0.50      0.29         2
       6010       0.50      0.50      0.50         2
       6011       0.00      0.00      0.00         2
       6012       0.20      0.50      0.29         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       0.50      0.50      0.50         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       1.00      1.00      1.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       1.00      1.00      1.00         2
       6085       0.00      0.00      0.00         2
       6087       0.00      0.00      0.00         2
       6090       0.50      0.50      0.50         2
       6097       0.33      0.50      0.40         2
       6101       1.00      0.50      0.67         2
       6103       0.00      0.00      0.00         2
       6108       1.00      1.00      1.00         2
       6112       0.50      0.50      0.50         2
       6123       0.67      1.00      0.80         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.40      1.00      0.57         2
       6136       0.00      0.00      0.00         2
       6145       0.40      1.00      0.57         2
       6159       0.00      0.00      0.00         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.20      0.50      0.29         2
       6182       0.17      0.50      0.25         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       0.50      0.50      0.50         2
       6210       1.00      1.00      1.00         2
       6211       1.00      1.00      1.00         2

avg / total       0.43      0.45      0.41       200


---------------------------------------------
all features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.445
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  82  114  139  179  206  263  276  322 3001 3042 3147 6005 6008 6011 6013
 6037 6053 6090 6103 6112 6129 6159 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  82  114  139  179  206  234  244  249  253  254  255  258  263  276  322
 3001 3042 3147 6005 6008 6011 6013 6015 6023 6033 6037 6053 6066 6076 6090
 6097 6103 6112 6129 6136 6159 6180 6181 6200]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       1.00      0.50      0.67         2
         20       0.50      0.50      0.50         2
         24       1.00      0.50      0.67         2
         29       0.17      0.50      0.25         2
         61       0.40      1.00      0.57         2
         75       0.20      0.50      0.29         2
         82       0.00      0.00      0.00         2
        114       0.00      0.00      0.00         2
        129       0.33      0.50      0.40         2
        130       1.00      1.00      1.00         2
        135       0.33      1.00      0.50         2
        137       0.40      1.00      0.57         2
        138       0.50      0.50      0.50         2
        139       0.00      0.00      0.00         2
        169       1.00      1.00      1.00         2
        179       0.00      0.00      0.00         2
        188       1.00      0.50      0.67         2
        194       0.50      1.00      0.67         2
        202       1.00      0.50      0.67         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.33      0.50      0.40         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       0.67      1.00      0.80         2
        249       0.00      0.00      0.00         2
        252       0.17      0.50      0.25         2
        253       0.00      0.00      0.00         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.00      0.00      0.00         2
        263       0.00      0.00      0.00         2
        276       0.00      0.00      0.00         2
        280       0.25      0.50      0.33         2
        294       1.00      0.50      0.67         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.25      1.00      0.40         2
        312       0.50      0.50      0.50         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.25      0.50      0.33         2
        415       0.50      0.50      0.50         2
       3001       0.00      0.00      0.00         2
       3006       0.67      1.00      0.80         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       0.50      1.00      0.67         2
       3032       0.67      1.00      0.80         2
       3042       0.00      0.00      0.00         2
       3064       1.00      1.00      1.00         2
       3091       1.00      0.50      0.67         2
       3092       0.25      0.50      0.33         2
       3125       1.00      1.00      1.00         2
       3147       0.00      0.00      0.00         2
       6005       0.00      0.00      0.00         2
       6006       1.00      0.50      0.67         2
       6008       0.00      0.00      0.00         2
       6010       1.00      1.00      1.00         2
       6011       0.00      0.00      0.00         2
       6012       0.17      0.50      0.25         2
       6013       0.00      0.00      0.00         2
       6015       0.00      0.00      0.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       0.00      0.00      0.00         2
       6053       0.00      0.00      0.00         2
       6066       0.00      0.00      0.00         2
       6076       0.00      0.00      0.00         2
       6082       1.00      0.50      0.67         2
       6085       0.50      0.50      0.50         2
       6087       1.00      0.50      0.67         2
       6090       0.00      0.00      0.00         2
       6097       0.00      0.00      0.00         2
       6101       1.00      0.50      0.67         2
       6103       0.00      0.00      0.00         2
       6108       1.00      1.00      1.00         2
       6112       0.00      0.00      0.00         2
       6123       0.22      1.00      0.36         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.33      1.00      0.50         2
       6136       0.00      0.00      0.00         2
       6145       0.50      0.50      0.50         2
       6159       0.00      0.00      0.00         2
       6171       0.67      1.00      0.80         2
       6180       0.00      0.00      0.00         2
       6181       0.00      0.00      0.00         2
       6182       0.20      0.50      0.29         2
       6197       0.50      0.50      0.50         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       0.50      0.50      0.50         2
       6210       0.50      0.50      0.50         2
       6211       0.50      0.50      0.50         2

avg / total       0.41      0.45      0.40       200


---------------------------------------------
TREE SELECTION features
---------------------------------------------

Best parameters:  {'n_neighbors': 1}

0-1 loss 0.59
Baseline 0.01

Detailed classification report:


/Users/lasseborgholt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [  19  206  253  263 6010 6013 6033 6097 6112 6129 6200]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [  19  206  234  249  253  255  263  312  322  415 6010 6013 6023 6033 6097
 6112 6129 6200]. 
  average=None)
             precision    recall  f1-score   support

          3       1.00      0.50      0.67         2
         19       0.00      0.00      0.00         2
         20       0.50      0.50      0.50         2
         24       0.50      0.50      0.50         2
         29       0.40      1.00      0.57         2
         61       1.00      0.50      0.67         2
         75       0.67      1.00      0.80         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.20      0.50      0.29         2
        130       1.00      1.00      1.00         2
        135       0.50      1.00      0.67         2
        137       0.40      1.00      0.57         2
        138       0.50      0.50      0.50         2
        139       1.00      0.50      0.67         2
        169       1.00      0.50      0.67         2
        179       1.00      0.50      0.67         2
        188       1.00      0.50      0.67         2
        194       0.50      1.00      0.67         2
        202       0.50      0.50      0.50         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       1.00      0.50      0.67         2
        234       0.00      0.00      0.00         2
        244       0.33      0.50      0.40         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       0.25      1.00      0.40         2
        253       0.00      0.00      0.00         2
        254       1.00      0.50      0.67         2
        255       0.00      0.00      0.00         2
        258       0.33      0.50      0.40         2
        263       0.00      0.00      0.00         2
        276       1.00      0.50      0.67         2
        280       0.67      1.00      0.80         2
        294       0.67      1.00      0.80         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.25      1.00      0.40         2
        312       0.00      0.00      0.00         2
        314       1.00      1.00      1.00         2
        320       0.50      1.00      0.67         2
        322       0.00      0.00      0.00         2
        358       1.00      0.50      0.67         2
        388       0.50      0.50      0.50         2
        415       0.00      0.00      0.00         2
       3001       1.00      0.50      0.67         2
       3006       0.50      1.00      0.67         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       0.33      1.00      0.50         2
       3042       1.00      0.50      0.67         2
       3064       1.00      0.50      0.67         2
       3091       1.00      1.00      1.00         2
       3092       0.33      1.00      0.50         2
       3125       1.00      0.50      0.67         2
       3147       1.00      0.50      0.67         2
       6005       1.00      0.50      0.67         2
       6006       0.50      0.50      0.50         2
       6008       0.50      0.50      0.50         2
       6010       0.00      0.00      0.00         2
       6011       0.50      0.50      0.50         2
       6012       0.33      0.50      0.40         2
       6013       0.00      0.00      0.00         2
       6015       1.00      0.50      0.67         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       1.00      1.00      1.00         2
       6066       1.00      1.00      1.00         2
       6076       1.00      1.00      1.00         2
       6082       1.00      0.50      0.67         2
       6085       0.50      0.50      0.50         2
       6087       1.00      0.50      0.67         2
       6090       1.00      0.50      0.67         2
       6097       0.00      0.00      0.00         2
       6101       1.00      0.50      0.67         2
       6103       1.00      0.50      0.67         2
       6108       1.00      1.00      1.00         2
       6112       0.00      0.00      0.00         2
       6123       0.25      1.00      0.40         2
       6124       1.00      1.00      1.00         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.67      1.00      0.80         2
       6159       1.00      0.50      0.67         2
       6171       0.67      1.00      0.80         2
       6180       0.50      0.50      0.50         2
       6181       0.33      0.50      0.40         2
       6182       0.50      1.00      0.67         2
       6197       1.00      0.50      0.67         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       0.50      0.50      0.50         2

avg / total       0.63      0.59      0.57       200


*********************************************
PCA results: explained variance = 95 %
*********************************************

---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 250
Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.888257575758
Baseline 0.717803030303

Detailed classification report:

             precision    recall  f1-score   support

          0       0.81      0.79      0.80       149
          1       0.92      0.93      0.92       379

avg / total       0.89      0.89      0.89       528


---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 249
Best parameters:  {'C': 2, 'gamma': 0.0001}

0-1 loss 0.650853889943
Baseline 0.529411764706

Detailed classification report:

             precision    recall  f1-score   support

          0       0.68      0.48      0.56       248
          1       0.63      0.80      0.71       279

avg / total       0.66      0.65      0.64       527


---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 250
Best parameters:  {'n_neighbors': 3}

0-1 loss 0.426136363636
Baseline 0.295454545455

Detailed classification report:

             precision    recall  f1-score   support

          1       0.45      0.46      0.46       156
          2       0.35      0.40      0.37       149
          3       0.36      0.30      0.32       105
          4       0.56      0.53      0.54       118

avg / total       0.43      0.43      0.43       528


---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 130
Best parameters:  {'C': 8, 'gamma': 0.001}

0-1 loss 0.875
Baseline 0.05

Detailed classification report:

             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         29       1.00      1.00      1.00         2
         75       1.00      1.00      1.00         2
        137       1.00      1.00      1.00         2
        202       1.00      0.50      0.67         2
        244       0.50      0.50      0.50         2
        249       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        258       0.50      0.50      0.50         2
        311       0.67      1.00      0.80         2
       3006       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       6008       0.50      1.00      0.67         2
       6131       1.00      1.00      1.00         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6210       1.00      0.50      0.67         2
       6211       1.00      1.00      1.00         2

avg / total       0.91      0.88      0.87        40


---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 108
Best parameters:  {'C': 4, 'gamma': 0.001}

0-1 loss 0.825
Baseline 0.05

Detailed classification report:

             precision    recall  f1-score   support

         19       1.00      1.00      1.00         2
         24       1.00      1.00      1.00         2
         29       0.29      1.00      0.44         2
         61       1.00      0.50      0.67         2
        129       1.00      0.50      0.67         2
        130       1.00      0.50      0.67         2
        252       1.00      1.00      1.00         2
        280       0.00      0.00      0.00         2
        294       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        322       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3125       0.67      1.00      0.80         2
       3147       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6203       1.00      1.00      1.00         2
       6204       1.00      0.50      0.67         2
       6211       1.00      1.00      1.00         2

avg / total       0.90      0.82      0.83        40


---------------------------------------------
all features
---------------------------------------------

Using PCA. No. of PC: 222
Best parameters:  {'C': 128, 'gamma': 0.0001}

0-1 loss 0.685
Baseline 0.01

Detailed classification report:

             precision    recall  f1-score   support

          3       1.00      1.00      1.00         2
         19       1.00      1.00      1.00         2
         20       1.00      0.50      0.67         2
         24       1.00      1.00      1.00         2
         29       0.25      1.00      0.40         2
         61       1.00      1.00      1.00         2
         75       0.33      0.50      0.40         2
         82       1.00      0.50      0.67         2
        114       1.00      0.50      0.67         2
        129       0.50      0.50      0.50         2
        130       1.00      1.00      1.00         2
        135       0.67      1.00      0.80         2
        137       1.00      1.00      1.00         2
        138       1.00      0.50      0.67         2
        139       1.00      1.00      1.00         2
        169       1.00      1.00      1.00         2
        179       1.00      0.50      0.67         2
        188       0.50      1.00      0.67         2
        194       1.00      1.00      1.00         2
        202       1.00      1.00      1.00         2
        203       1.00      1.00      1.00         2
        206       0.00      0.00      0.00         2
        228       0.50      0.50      0.50         2
        234       0.00      0.00      0.00         2
        244       0.00      0.00      0.00         2
        245       1.00      1.00      1.00         2
        246       1.00      1.00      1.00         2
        249       0.00      0.00      0.00         2
        252       0.67      1.00      0.80         2
        253       0.50      0.50      0.50         2
        254       0.00      0.00      0.00         2
        255       0.00      0.00      0.00         2
        258       0.20      0.50      0.29         2
        263       0.00      0.00      0.00         2
        276       0.50      0.50      0.50         2
        280       1.00      1.00      1.00         2
        294       0.67      1.00      0.80         2
        302       1.00      1.00      1.00         2
        303       1.00      1.00      1.00         2
        311       0.14      0.50      0.22         2
        312       1.00      1.00      1.00         2
        314       1.00      1.00      1.00         2
        320       1.00      1.00      1.00         2
        322       1.00      0.50      0.67         2
        358       1.00      0.50      0.67         2
        388       1.00      0.50      0.67         2
        415       0.50      1.00      0.67         2
       3001       1.00      0.50      0.67         2
       3006       1.00      1.00      1.00         2
       3008       1.00      1.00      1.00         2
       3013       1.00      1.00      1.00         2
       3019       1.00      1.00      1.00         2
       3032       1.00      1.00      1.00         2
       3042       1.00      1.00      1.00         2
       3064       0.50      1.00      0.67         2
       3091       1.00      0.50      0.67         2
       3092       0.33      0.50      0.40         2
       3125       1.00      1.00      1.00         2
       3147       0.67      1.00      0.80         2
       6005       1.00      0.50      0.67         2
       6006       1.00      0.50      0.67         2
       6008       0.25      1.00      0.40         2
       6010       1.00      0.50      0.67         2
       6011       0.00      0.00      0.00         2
       6012       0.20      0.50      0.29         2
       6013       0.00      0.00      0.00         2
       6015       1.00      1.00      1.00         2
       6016       1.00      1.00      1.00         2
       6023       0.00      0.00      0.00         2
       6033       0.00      0.00      0.00         2
       6037       1.00      0.50      0.67         2
       6053       1.00      1.00      1.00         2
       6066       1.00      0.50      0.67         2
       6076       1.00      1.00      1.00         2
       6082       1.00      1.00      1.00         2
       6085       0.50      0.50      0.50         2
       6087       1.00      0.50      0.67         2
       6090       1.00      1.00      1.00         2
       6097       0.33      0.50      0.40         2
       6101       0.50      0.50      0.50         2
       6103       0.50      0.50      0.50         2
       6108       0.67      1.00      0.80         2
       6112       1.00      1.00      1.00         2
       6123       1.00      1.00      1.00         2
       6124       1.00      0.50      0.67         2
       6129       0.00      0.00      0.00         2
       6131       0.67      1.00      0.80         2
       6136       0.50      0.50      0.50         2
       6145       0.50      1.00      0.67         2
       6159       1.00      1.00      1.00         2
       6171       1.00      1.00      1.00         2
       6180       0.00      0.00      0.00         2
       6181       0.50      0.50      0.50         2
       6182       1.00      1.00      1.00         2
       6197       1.00      1.00      1.00         2
       6200       0.00      0.00      0.00         2
       6203       0.50      0.50      0.50         2
       6204       1.00      0.50      0.67         2
       6210       1.00      1.00      1.00         2
       6211       1.00      0.50      0.67         2

avg / total       0.71      0.69      0.67       200